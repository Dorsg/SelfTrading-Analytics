
===== backend/analytics/sim_scheduler.py =====

DESCRIPTION: Source file

from __future__ import annotations

import asyncio
import json
import logging
import os
from datetime import datetime, timezone, timedelta, date, time

from logger_config import setup_logging  # ← ensure file handlers & levels
from database.db_manager import DBManager
from database.models import SimulationState
from backend.analytics.runner_service import RunnerService
from backend.ib_manager.market_data_manager import MarketDataManager

# Configure logging for this process
setup_logging()
log = logging.getLogger("AnalyticsScheduler")

PACE_FILE = "/tmp/sim_auto_advance.json"
HEARTBEAT_FILE = "/tmp/sim_scheduler.heartbeat"

# ──────────────────────────────────────────────────────────────────────────────
# Tunables (sane defaults; all overridable via env)
# ──────────────────────────────────────────────────────────────────────────────

# Minimum fully-completed bars strategies expect (RSI14/MA/Fib/Donchian windows).
# Keep this at 21 unless you change strategy periods materially.
MIN_REQUIRED_BARS = int(os.getenv("SIM_MIN_REQUIRED_BARS", "21"))

# Default: use the next real 5m market candle as our step; still keep this for warmup math.
def _step_seconds() -> int:
    return int(os.getenv("SIM_STEP_SECONDS", "300"))  # 5 minutes per tick


def _read_pace_seconds() -> float:
    try:
        if os.path.exists(PACE_FILE):
            with open(PACE_FILE, "r") as f:
                data = json.load(f)
                if not data.get("enabled", True):
                    # If disabled, sleep a little so we don't hot-spin the loop
                    return 0.5
                pace = float(data.get("pace_seconds", 0.0))
                return max(0.0, pace)
    except Exception:
        pass
    return float(os.getenv("SIM_PACE_SECONDS", "0"))  # default: run at full speed


def _warmup_bars_default() -> int:
    """
    Bars to skip from the global min 5m timestamp so strategies have enough data.
    Default = 50 (> 21) so first indicator windows are fully stable.
    Override with SIM_WARMUP_BARS or WARMUP_BARS.
    """
    return int(os.getenv("SIM_WARMUP_BARS", os.getenv("WARMUP_BARS", "50")))


def _daily_warmup_days_default() -> int:
    """
    Extra guard for DAILY runners: start the whole sim only after at least this
    many daily candles exist globally. Default 30 days. Override via:
      SIM_DAILY_WARMUP_DAYS or DAILY_WARMUP_DAYS
    """
    return int(os.getenv("SIM_DAILY_WARMUP_DAYS", os.getenv("DAILY_WARMUP_DAYS", "30")))


def _session_warmup_bars_default() -> int:
    """
    Number of 5m bars to have **after NYSE open** in the *current day* before we run strategies.
    We require MIN_REQUIRED_BARS completed bars **plus one** to avoid off-by-one when a bar
    is still forming or the data provider is mid-commit.
    Default 22; override via SIM_SESSION_WARMUP_BARS or SESSION_WARMUP_BARS.
    """
    fallback = max(MIN_REQUIRED_BARS + 1, 22)
    return int(os.getenv("SIM_SESSION_WARMUP_BARS", os.getenv("SESSION_WARMUP_BARS", str(fallback))))


def _ny_open_epoch_for_day(dt_utc: datetime) -> int:
    """
    Return the UTC epoch for 09:30 ET on the ET calendar date of dt_utc.
    """
    dt_utc = dt_utc if dt_utc.tzinfo else dt_utc.replace(tzinfo=timezone.utc)
    try:
        from zoneinfo import ZoneInfo  # type: ignore
        ny = ZoneInfo("America/New_York")
        et = dt_utc.astimezone(ny)
        et_day = et.date()
        open_et = datetime(et_day.year, et_day.month, et_day.day, 9, 30, tzinfo=ny)
        open_utc = open_et.astimezone(timezone.utc)
        return int(open_utc.timestamp())
    except Exception:
        # Conservative fallback if zoneinfo unavailable: 13:30 UTC ≈ 09:30 ET (no DST correction)
        approx = dt_utc.replace(hour=13, minute=30, second=0, microsecond=0, tzinfo=timezone.utc)
        return int(approx.timestamp())


async def _heartbeat() -> None:
    try:
        with open(HEARTBEAT_FILE, "w") as f:
            f.write(datetime.now(timezone.utc).isoformat())
    except Exception:
        pass


async def _advance_one_tick(rs: RunnerService, ts: int) -> tuple[int, dict]:
    # NOTE: this is now controlled by session-aware stepping outside; we keep the signature
    stats = await rs.run_tick(datetime.fromtimestamp(ts, tz=timezone.utc))
    return ts, stats  # next epoch chosen separately


def _ts(dt: datetime | None) -> int | None:
    if not dt:
        return None
    return int((dt if dt.tzinfo else dt.replace(tzinfo=timezone.utc)).timestamp())


async def main() -> None:
    # Ensure tiny migrations also run when the scheduler/runner is started without the API process.
    try:
        from backend.database.init_db import _apply_light_migrations
        _apply_light_migrations()
    except Exception:
        log.exception("Failed to apply light migrations at scheduler startup")

    tick_log_every = max(1, int(os.getenv("TICK_LOG_EVERY", "1")))
    boundary_refresh_ticks = int(os.getenv("SIM_BOUNDARY_REFRESH_TICKS", "0"))  # 0 = never refresh

    rs = RunnerService()
    mkt = MarketDataManager()

    # Decide the session clock symbol up-front (resilient)
    step_sec = _step_seconds()
    tf_min = step_sec // 60
    requested_clock = os.getenv("SIM_REFERENCE_CLOCK_SYMBOL", "SPY").upper()
    clock_sym = requested_clock
    if not mkt.has_minute_bars(clock_sym, tf_min):
        picked = mkt.pick_reference_symbol(interval_min=tf_min)
        if picked:
            log.info(
                "Session clock '%s' unavailable at %dm — switching to '%s'. "
                "Provide SIM_REFERENCE_CLOCK_SYMBOL to override.",
                requested_clock, tf_min, picked
            )
            clock_sym = picked
        else:
            log.warning(
                "No obvious clock symbol found for %dm. Will rely on GLOBAL fallback (any symbol).",
                tf_min
            )

    log.info(
        "Simulation scheduler loop started. LOG_LEVEL=%s  tick_log_every=%s  boundary_refresh_ticks=%s  clock_symbol=%s",
        os.getenv("LOG_LEVEL", "DEBUG"),
        tick_log_every,
        boundary_refresh_ticks,
        (clock_sym or "<global>"),
    )

    # Log 5m data boundaries once at startup (and fetch daily min date)
    min_5m_dt = max_5m_dt = min_daily_dt = None
    try:
        from sqlalchemy import select, func
        from database.models import HistoricalMinuteBar, HistoricalDailyBar
        with DBManager() as db:
            with db.db.bind.connect() as conn:  # type: ignore[attr-defined]
                min_5m_dt = conn.execute(select(func.min(HistoricalMinuteBar.ts))).scalar()
                max_5m_dt = conn.execute(select(func.max(HistoricalMinuteBar.ts))).scalar()
                min_daily_dt = conn.execute(select(func.min(HistoricalDailyBar.date))).scalar()
        log.info("Historical 5m data range: start=%s end=%s", min_5m_dt, max_5m_dt)
    except Exception:
        log.exception("Failed to log historical range at startup")

    # Cached boundaries reused across ticks; optionally refreshed by env
    cached_min_ts = min_5m_dt
    cached_max_ts = max_5m_dt
    cached_min_daily = min_daily_dt

    # ── Internal monotonic clock (source of truth for advancing) ──
    state_epoch: int | None = None  # seconds since epoch, UTC

    tick = 0
    while True:
        pace = _read_pace_seconds()
        try:
            await _heartbeat()

            from sqlalchemy import select, func, text
            from database.models import HistoricalMinuteBar, HistoricalDailyBar

            with DBManager() as db:
                user = db.get_user_by_username("analytics")
                if not user:
                    await asyncio.sleep(1.0)
                    continue

                uid = int(getattr(user, "id"))
                st = db.db.query(SimulationState).filter(SimulationState.user_id == uid).first()
                if not st:
                    st = SimulationState(user_id=uid, is_running="false")
                    db.db.add(st)
                    db.db.commit()
                    await asyncio.sleep(1.0)
                    continue

                if str(st.is_running).lower() not in {"true", "1"}:
                    if tick % 10 == 0:
                        log.debug("Idle: simulation not running")
                    await asyncio.sleep(1.0)
                    tick += 1
                    continue

                # ── Boundaries: optionally refresh on a cadence ───────────────
                if (
                    cached_min_ts is None or
                    cached_max_ts is None or
                    (boundary_refresh_ticks > 0 and tick % boundary_refresh_ticks == 0)
                ):
                    with db.db.bind.connect() as conn:  # type: ignore[attr-defined]
                        cached_min_ts = conn.execute(select(func.min(HistoricalMinuteBar.ts))).scalar()
                        cached_max_ts = conn.execute(select(func.max(HistoricalMinuteBar.ts))).scalar()
                        cached_min_daily = conn.execute(select(func.min(HistoricalDailyBar.date))).scalar()

                if not cached_min_ts or not cached_max_ts:
                    log.warning("No minute bars present; pausing run.")
                    await asyncio.sleep(1.0)
                    tick += 1
                    continue

                step_sec = _step_seconds()
                warmup_bars = _warmup_bars_default()
                daily_warmup_days = _daily_warmup_days_default()
                session_warmup_bars = _session_warmup_bars_default()

                min_epoch = int(cached_min_ts.replace(tzinfo=timezone.utc).timestamp())
                max_epoch = int(cached_max_ts.replace(tzinfo=timezone.utc).timestamp())

                # Base: earliest 5m + warmup bars (raw)
                base_start_epoch = min_epoch + warmup_bars * step_sec

                # Daily guard: earliest daily + N days
                if cached_min_daily:
                    min_daily_epoch = int(cached_min_daily.replace(tzinfo=timezone.utc).timestamp())
                    daily_guard_epoch = min_daily_epoch + daily_warmup_days * 86400
                    desired_start = max(base_start_epoch, daily_guard_epoch)
                else:
                    desired_start = base_start_epoch

                # Align to the **next real market candle** (preferred clock or global fallback)
                aligned_dt = mkt.get_next_session_ts(
                    datetime.fromtimestamp(desired_start, tz=timezone.utc),
                    interval_min=step_sec // 60,
                    reference_symbol=clock_sym if clock_sym else None,
                )
                if aligned_dt is None:
                    aligned_dt = mkt.get_next_session_ts_global(
                        datetime.fromtimestamp(desired_start, tz=timezone.utc),
                        interval_min=step_sec // 60,
                    )
                if aligned_dt is not None:
                    desired_start = min(int(aligned_dt.timestamp()), max_epoch)

                # Normalize DB value (if any)
                db_epoch = _ts(st.last_ts)

                # Initialize internal clock once
                if state_epoch is None:
                    base = db_epoch if (db_epoch is not None) else desired_start
                    base_dt = datetime.fromtimestamp(min(max(base, desired_start), max_epoch), tz=timezone.utc)
                    next_dt = mkt.get_next_session_ts(
                        base_dt,
                        interval_min=step_sec // 60,
                        reference_symbol=clock_sym if clock_sym else None,
                    )
                    if next_dt is None:
                        next_dt = mkt.get_next_session_ts_global(base_dt, interval_min=step_sec // 60)
                    if next_dt is None:
                        st.is_running = "false"
                        db.db.commit()
                        log.info("No session ticks available at/after %s. Stopping.", base_dt.isoformat())
                        await asyncio.sleep(1.0)
                        tick += 1
                        continue

                    state_epoch = int(next_dt.timestamp())

                    # Per-day session warmup anchored to **NY open**
                    open_epoch = _ny_open_epoch_for_day(next_dt)
                    warmup_epoch = open_epoch + session_warmup_bars * step_sec
                    if state_epoch < warmup_epoch <= max_epoch:
                        log.debug(
                            "Session warmup: skipping to %s after NY open (%d bars).",
                            datetime.fromtimestamp(warmup_epoch, tz=timezone.utc).isoformat(),
                            session_warmup_bars,
                        )
                        state_epoch = warmup_epoch

                    # Persist start monotonically
                    target_dt = datetime.fromtimestamp(state_epoch, tz=timezone.utc)
                    db.db.execute(
                        text(
                            "UPDATE simulation_state "
                            "   SET last_ts = CASE WHEN last_ts IS NULL OR last_ts < :ts THEN :ts ELSE last_ts END "
                            " WHERE user_id = :uid"
                        ),
                        {"ts": target_dt, "uid": uid},
                    )
                    db.db.commit()

                    st.last_ts = target_dt
                    log.info(
                        "Initialized simulation clock: db_epoch=%s desired_start(aligned)=%s -> start_at=%s (clock=%s)",
                        db_epoch, desired_start, st.last_ts.isoformat(), (clock_sym or "<global>")
                    )
                else:
                    # Re-read fresh DB value for comparison
                    db_epoch = _ts(
                        db.db.query(SimulationState.last_ts)
                        .filter(SimulationState.user_id == uid)
                        .scalar()
                    )

                    # Guard against DB regression (someone wrote an earlier ts)
                    if db_epoch is not None and (db_epoch + step_sec) < state_epoch:
                        log.warning(
                            "Detected DB last_ts regression (%s < %s). Overwriting with monotonic clock.",
                            db_epoch, state_epoch
                        )
                        target_dt = datetime.fromtimestamp(state_epoch, tz=timezone.utc)
                        db.db.execute(
                            text(
                                "UPDATE simulation_state "
                                "   SET last_ts = CASE WHEN last_ts IS NULL OR last_ts < :ts THEN :ts ELSE last_ts END "
                                " WHERE user_id = :uid"
                            ),
                            {"ts": target_dt, "uid": uid},
                        )
                        db.db.commit()

                    # If DB jumped forward (manual fast-forward), adopt it (align to next session)
                    if db_epoch is not None and db_epoch > state_epoch + step_sec:
                        log.info(
                            "Adopting DB fast-forward: state_epoch=%s -> db_epoch=%s",
                            state_epoch, db_epoch
                        )
                        jump_dt = datetime.fromtimestamp(db_epoch, tz=timezone.utc)
                        next_dt = mkt.get_next_session_ts(
                            jump_dt,
                            interval_min=step_sec // 60,
                            reference_symbol=clock_sym if clock_sym else None,
                        )
                        if next_dt is None:
                            next_dt = mkt.get_next_session_ts_global(jump_dt, interval_min=step_sec // 60)
                        if next_dt is None:
                            st.is_running = "false"
                            db.db.commit()
                            log.info("No session ticks available at/after %s. Stopping.", jump_dt.isoformat())
                            await asyncio.sleep(1.0)
                            tick += 1
                            continue
                        state_epoch = int(next_dt.timestamp())

                # End-of-data check
                if state_epoch >= max_epoch:
                    st.is_running = "false"
                    db.db.commit()
                    log.info("Reached end of historical data (%s). Stopping simulation.", cached_max_ts.isoformat())
                    await asyncio.sleep(1.0)
                    tick += 1
                    continue

                # ── Session-aware selection of the current tick ────────────────
                cur_dt = datetime.fromtimestamp(state_epoch, tz=timezone.utc)

                # Advance **one** tick (the one at state_epoch)
                cur_ts, stats = await _advance_one_tick(rs, state_epoch)

                # Compute the **next** market-session tick strictly after cur_dt
                next_dt = mkt.get_next_session_ts(
                    cur_dt,
                    interval_min=_step_seconds() // 60,
                    reference_symbol=clock_sym if clock_sym else None,
                )
                if next_dt is None:
                    next_dt = mkt.get_next_session_ts_global(cur_dt, interval_min=_step_seconds() // 60)
                if next_dt is None:
                    st.is_running = "false"
                    db.db.commit()
                    log.info("No further session ticks after %s. Stopping simulation.", cur_dt.isoformat())
                    await asyncio.sleep(1.0)
                    tick += 1
                    continue

                state_epoch = int(next_dt.timestamp())

                # Persist next_ts monotonically
                target_dt = datetime.fromtimestamp(state_epoch, tz=timezone.utc)
                db.db.execute(
                    text(
                        "UPDATE simulation_state "
                        "   SET last_ts = CASE WHEN last_ts IS NULL OR last_ts < :ts THEN :ts ELSE last_ts END "
                        " WHERE user_id = :uid"
                    ),
                    {"ts": target_dt, "uid": uid},
                )
                db.db.commit()
                st.last_ts = target_dt

                # ── Progress indicator ───────────────
                start_epoch = int(desired_start)
                total_span = max(1, max_epoch - start_epoch)
                done_span = max(0, cur_ts - start_epoch)
                pct = max(0.0, min(100.0, (done_span / total_span) * 100.0))

                if tick % tick_log_every == 0:
                    as_of_s = datetime.fromtimestamp(cur_ts, tz=timezone.utc).isoformat()
                    next_s = datetime.fromtimestamp(state_epoch, tz=timezone.utc).isoformat()
                    pace_label = "full-speed" if pace <= 0 else f"{pace:.2f}s delay"
                    log.debug(
                        "TICK #%d as_of=%s → next=%s | runners: processed=%d buys=%d sells=%d "
                        "no_action=%d skipped_no_data=%d skipped_no_budget=%d errors=%d | "
                        "progress=%.4f%% (session-aware; clock=%s; pace=%s)",
                        tick,
                        as_of_s,
                        next_s,
                        int(stats.get("processed", 0)),
                        int(stats.get("buys", 0)),
                        int(stats.get("sells", 0)),
                        int(stats.get("no_action", 0)),
                        int(stats.get("skipped_no_data", 0)),
                        int(stats.get("skipped_no_budget", 0)),
                        int(stats.get("errors", 0)),
                        pct,
                        (clock_sym or "<global>"),
                        pace_label,
                    )

                await asyncio.sleep(pace if pace > 0 else 0)
                tick += 1

        except Exception:
            log.exception("Scheduler loop error")
            # brief back-off so we don't spin on same exception
            await asyncio.sleep(0.5)
            tick += 1


if __name__ == "__main__":
    asyncio.run(main())


===== backend/analytics/runner_service.py =====

DESCRIPTION: Source file

from __future__ import annotations

import os
import json
import logging
from dataclasses import dataclass
from datetime import datetime, timezone, timedelta
from typing import Optional, List, Dict, Any, Tuple, Set


from database.db_manager import DBManager
from database.models import Runner, OpenPosition, User
from backend.ib_manager.market_data_manager import MarketDataManager
from backend.broker.mock_broker import MockBroker
from backend.strategies.runner_decision_info import RunnerDecisionInfo
from backend.strategies.factory import select_strategy, resolve_strategy_key
from backend.strategies.contracts import validate_decision

log = logging.getLogger("runner-service")


@dataclass(slots=True)
class _RunnerCtx:
    runner: Any
    position: Optional[OpenPosition]
    price: float
    candles: List[Dict[str, Any]]


@dataclass(slots=True)
class RunnerView:
    id: int
    user_id: int
    name: str
    strategy: str
    budget: float
    current_budget: float
    stock: str
    time_frame: int
    parameters: dict
    exit_strategy: str
    activation: str


class RunnerService:
    """
    Executes one decision tick across active runners.

    Exit rules enforced:
    • Hold after BUY until either:
        (1) broker stop hits (MockBroker.on_tick closes and logs once), or
        (2) strategy SELL triggers (we call MockBroker.sell_all, which logs).
    • The trailing stop is broker-managed and is armed once at BUY.
      Strategy shouldn't cause immediate sell-only-because-a-trailing-stop-exists.

    Additional safeguards:
    • Skip trading when the latest candle is stale (older than TF or from a prior day).
    • NEW: Only honor strategy-driven BUY/SELL when the candle/bar has advanced
      since the last action we evaluated for that runner+timeframe. Broker stops
      remain evaluated every tick.
    """

    def __init__(self) -> None:
        self.mkt = MarketDataManager()
        self.broker = MockBroker()
        self._cache_seq: Optional[int] = None
        self._candle_cache: Dict[Tuple[str, int, int], List[Dict[str, Any]]] = {}

        self._log_no_action = os.getenv("SIM_LOG_NO_ACTION", "0") == "1"
        self._thin_no_action_details = os.getenv("SIM_THIN_NO_ACTION_DETAILS", "1") == "1"

        # Fixed per-runner budget (env override supported)
        self._unit_budget_usd = float(os.getenv("SIM_RUNNER_UNIT_BUDGET", "2000"))

        # Minimum cash safeguard for simulator
        self._min_cash_floor = float(os.getenv("SIM_MIN_CASH", "5000000"))
        self._topup_cash_to = float(os.getenv("SIM_TOPUP_CASH_TO", "10000000"))

        # Stale price guard (always on by default)
        self._skip_stale_price = os.getenv("SIM_SKIP_STALE_PRICE", "1") == "1"

        # NEW: Require bar advancement for strategy-driven entries/exits
        self._require_bar_advance = os.getenv("SIM_REQUIRE_BAR_ADVANCE", "1") == "1"

        # NEW: Remember last processed candle TS per (runner_id, timeframe)
        self._last_bar_ts: Dict[Tuple[int, int], datetime] = {}

        # NEW: Only use regular-hours candles by default (can be disabled)
        self._regular_hours_only = os.getenv("SIM_REGULAR_HOURS_ONLY", "1") == "1"

        # NEW: Warn once per (symbol, tf, ET date) if no candles available
        self._warn_no_data_once: Set[Tuple[str, int, str]] = set()

    # ───────────────────────── internals ─────────────────────────
    def _get_candles_cached(
        self,
        symbol: str,
        interval_min: int,
        as_of: datetime,
        lookback: int = 300
    ) -> List[Dict[str, Any]]:
        as_of = as_of.astimezone(timezone.utc)
        seq = int(as_of.timestamp())
        sym = symbol.upper()
        key = (sym, int(interval_min), seq)

        if self._cache_seq != seq:
            self._cache_seq = seq
            self._candle_cache.clear()

        if key in self._candle_cache:
            return self._candle_cache[key]

        candles = self.mkt.get_candles_until(
            sym,
            int(interval_min),
            as_of,
            lookback=lookback,
            regular_hours_only=self._regular_hours_only,
        )
        self._candle_cache[key] = candles
        return candles


    def _prefetch_candles_for_runners(self, runners: List[RunnerView], as_of: datetime) -> None:
        if not runners:
            return
        as_of = as_of.astimezone(timezone.utc)
        seq = int(as_of.timestamp())

        self._cache_seq = seq
        self._candle_cache.clear()

        syms_5 = sorted({r.stock for r in runners if int(r.time_frame or 5) == 5})
        syms_1d = sorted({r.stock for r in runners if int(r.time_frame or 5) == 1440})

        if syms_5:
            data5 = self.mkt.get_candles_bulk_until(
                syms_5, 5, as_of, lookback=300, regular_hours_only=self._regular_hours_only
            )
            for s, candles in data5.items():
                self._candle_cache[(s, 5, seq)] = candles
        if syms_1d:
            data1d = self.mkt.get_candles_bulk_until(
                syms_1d, 1440, as_of, lookback=300, regular_hours_only=False  # daily unaffected by RTH filter
            )
            for s, candles in data1d.items():
                self._candle_cache[(s, 1440, seq)] = candles

        for s in syms_5:
            self._candle_cache.setdefault((s, 5, seq), [])
        for s in syms_1d:
            self._candle_cache.setdefault((s, 1440, seq), [])


    @staticmethod
    def _last_candle_ts(candles: List[Dict[str, Any]]) -> Optional[datetime]:
        if not candles:
            return None
        ts = candles[-1].get("ts")
        if ts is None:
            return None
        # Historical tables already store timezone-aware datetimes; ensure UTC
        return (ts if getattr(ts, "tzinfo", None) else ts.replace(tzinfo=timezone.utc)).astimezone(timezone.utc)

    @staticmethod
    def _is_stale_candle(last_ts: Optional[datetime], tf_min: int, as_of: datetime) -> bool:
        if last_ts is None:
            return True
        # If the last bar is from a prior day → stale (pre-open for that TF)
        if last_ts.date() < as_of.date():
            return True
        # If the last bar is older than one TF window → stale
        age_sec = (as_of - last_ts).total_seconds()
        return age_sec > (tf_min * 60 + 1)

    def _decide(self, ctx: _RunnerCtx, strategy_obj=None) -> dict:
        info = RunnerDecisionInfo(
            runner=ctx.runner,
            position=ctx.position,
            current_price=ctx.price,
            candles=ctx.candles,
            distance_from_time_limit=None,
        )
        strat = strategy_obj or select_strategy(ctx.runner)
        raw = strat.decide_buy(info) if ctx.position is None else strat.decide_sell(info)
        decision = validate_decision(raw, is_exit=ctx.position is not None) or {"action": "NO_ACTION"}

        # Inject a static stop at BUY if strategy didn't provide any stop
        if (decision.get("action") or "NO_ACTION").upper() == "BUY":
            has_trail = isinstance(decision.get("trail_stop_order"), dict)
            has_static = isinstance(decision.get("static_stop_order"), dict)
            if not has_trail and not has_static:
                try:
                    params = getattr(ctx.runner, "parameters", {}) or {}
                    sl_pct = float(params.get("default_stop_loss_percent", 0.0) or 0.0)
                except Exception:
                    sl_pct = 0.0
                if sl_pct > 0:
                    decision["static_stop_order"] = {
                        "action": "SELL",
                        "order_type": "STOP",
                        "stop_price": round(ctx.price * (1.0 - sl_pct / 100.0), 4),
                    }
        return decision

    def _qty_from_budget(self, db: DBManager, r: RunnerView, price: float) -> int:
        try:
            if price is None or price <= 0:
                return 0
            qty = int(self._unit_budget_usd // max(price, 0.01))
            return max(qty, 0)
        except Exception:
            return 0

    @staticmethod
    def _snapshot_runner(r: Runner) -> RunnerView:
        try:
            return RunnerView(
                id=int(getattr(r, "id")),
                user_id=int(getattr(r, "user_id")),
                name=str(getattr(r, "name", "") or ""),
                strategy=str(getattr(r, "strategy", "") or ""),
                budget=float(getattr(r, "budget", 0.0) or 0.0),
                current_budget=float(getattr(r, "current_budget", 0.0) or 0.0),
                stock=str(getattr(r, "stock", "UNKNOWN") or "UNKNOWN").upper(),
                time_frame=int(getattr(r, "time_frame", 5) or 5),
                parameters=dict(getattr(r, "parameters", {}) or {}),
                exit_strategy=str(getattr(r, "exit_strategy", "hold_forever") or "hold_forever"),
                activation=str(getattr(r, "activation", "active") or "active"),
            )
        except Exception:
            return RunnerView(
                id=int(getattr(r, "id", 0) or 0),
                user_id=int(getattr(r, "user_id", 0) or 0),
                name=str(getattr(r, "name", "") or ""),
                strategy=str(getattr(r, "strategy", "") or ""),
                budget=float(getattr(r, "budget", 0.0) or 0.0),
                current_budget=float(getattr(r, "current_budget", 0.0) or 0.0),
                stock=str(getattr(r, "stock", "UNKNOWN") or "UNKNOWN").upper(),
                time_frame=int(getattr(r, "time_frame", 5) or 5),
                parameters=dict(getattr(r, "parameters", {}) or {}),
                exit_strategy=str(getattr(r, "exit_strategy", "hold_forever") or "hold_forever"),
                activation=str(getattr(r, "activation", "active") or "active"),
            )

    # ───────────────────────── public ─────────────────────────
    async def run_tick(self, as_of: datetime) -> dict:
        as_of = as_of.astimezone(timezone.utc)
        seq = int(as_of.timestamp())

        stats = {
            "processed": 0,
            "buys": 0,
            "sells": 0,
            "no_action": 0,
            "skipped_no_data": 0,
            "skipped_no_budget": 0,
            "errors": 0,
        }

        exec_buffer: List[dict] = []

        with DBManager() as db:
            user = db.get_user_by_username("analytics")
            if not user:
                log.warning("No analytics user found yet.")
                return stats

            uid = int(getattr(user, "id"))

            # Ensure account exists and is funded for simulation
            try:
                acct = db.ensure_account(user_id=uid, name="mock")
                try:
                    current_cash = float(getattr(acct, "cash", 0.0) or 0.0)
                except Exception:
                    current_cash = 0.0
                if current_cash < self._min_cash_floor:
                    try:
                        setattr(acct, "cash", self._topup_cash_to)
                        db.db.commit()
                        log.info(
                            "Top-upped mock account cash to $%.2f for user_id=%s (previous=%.2f, floor=%.2f)",
                            self._topup_cash_to, uid, current_cash, self._min_cash_floor
                        )
                    except Exception:
                        log.exception("Failed to top-up mock account cash for user_id=%s", uid)
            except Exception:
                log.exception("ensure_account failed for user_id=%s", uid)

            runners_orm = db.get_runners_by_user(user_id=uid, activation="active")
            runners: List[RunnerView] = [self._snapshot_runner(r) for r in runners_orm]

            self._prefetch_candles_for_runners(runners, as_of)

            for r in runners:
                try:
                    rid = int(getattr(r, "id", 0) or 0)
                    if rid == 0:
                        exec_buffer.append({
                            "runner_id": 0,
                            "user_id": uid,
                            "symbol": (getattr(r, "stock", "") or "UNKNOWN").upper(),
                            "strategy": str(getattr(r, "strategy", "")),
                            "status": "skipped-invalid-runner",
                            "reason": "no_primary_key",
                            "details": json.dumps({"error": "runner row missing primary key"}, ensure_ascii=False),
                            "execution_time": as_of,
                            "cycle_seq": seq,
                        })
                        stats["no_action"] += 1
                        stats["processed"] += 1
                        continue

                    canon = resolve_strategy_key(getattr(r, "strategy", None))
                    if not canon:
                        exec_buffer.append({
                            "runner_id": r.id,
                            "user_id": uid,
                            "symbol": r.stock,
                            "strategy": str(getattr(r, "strategy", "")),
                            "status": "skipped-unknown-strategy",
                            "reason": "unknown_strategy",
                            "details": json.dumps({"strategy": getattr(r, "strategy", None)}, ensure_ascii=False),
                            "execution_time": as_of,
                            "cycle_seq": seq,
                        })
                        stats["no_action"] += 1
                        stats["processed"] += 1
                        continue

                    tf = int(getattr(r, "time_frame", 5) or 5)
                    candles = self._get_candles_cached(r.stock, tf, as_of, lookback=300)
                    if not candles:
                        # Warn once per (symbol, tf, ET date), but demote to INFO when symbol has no coverage at all (e.g., post-IPO for this sim date)
                        try:
                            from zoneinfo import ZoneInfo  # type: ignore
                            ny = ZoneInfo("America/New_York")
                            et_day = as_of.astimezone(ny).date().isoformat()
                        except Exception:
                            et_day = as_of.date().isoformat()
                        key = (r.stock, tf, et_day)

                        has_cov = self.mkt.has_daily_bars(r.stock) if tf == 1440 else self.mkt.has_minute_bars(r.stock, tf)
                        msg = f"No historical candles for {r.stock} tf={tf}m at {as_of.isoformat()} (regular_hours_only={self._regular_hours_only}, coverage={has_cov})"
                        if key not in self._warn_no_data_once:
                            self._warn_no_data_once.add(key)
                            if has_cov:
                                log.warning(msg)
                            else:
                                log.info(msg + " — likely pre-IPO or outside data coverage; skipping.")

                        exec_buffer.append({
                            "runner_id": r.id,
                            "user_id": uid,
                            "symbol": r.stock,
                            "strategy": r.strategy,
                            "status": "skipped-no-data",
                            "reason": "insufficient_candles",
                            "details": (None if self._thin_no_action_details else json.dumps({"message": "no candles available at as_of", "tf": tf}, ensure_ascii=False)),
                            "execution_time": as_of,
                            "cycle_seq": seq,
                        })
                        stats["skipped_no_data"] += 1
                        stats["processed"] += 1
                        continue

                    last_ts = self._last_candle_ts(candles)
                    is_stale = self._skip_stale_price and self._is_stale_candle(last_ts, tf, as_of)

                    # If stale, do NOT ping broker (so we don't evaluate stops on frozen prices)
                    if is_stale:
                        exec_buffer.append({
                            "runner_id": r.id,
                            "user_id": uid,
                            "symbol": r.stock,
                            "strategy": r.strategy,
                            "status": "completed",
                            "reason": "skipped-stale-price",
                            "details": (None if self._thin_no_action_details else json.dumps({
                                "message": "last candle is stale for timeframe",
                                "tf_min": tf,
                                "last_ts": (last_ts.isoformat() if last_ts else None),
                                "as_of": as_of.isoformat(),
                            }, ensure_ascii=False)),
                            "execution_time": as_of,
                            "cycle_seq": seq,
                        })
                        if self._log_no_action:
                            log.debug("NO_ACTION %s tf=%dm — stale candle (last_ts=%s, as_of=%s)",
                                      r.stock, tf, (last_ts.isoformat() if last_ts else "None"), as_of.isoformat())
                        stats["no_action"] += 1
                        stats["processed"] += 1
                        continue

                    # Fresh price → broker first (so stops can close if truly hit)
                    price = float(candles[-1]["close"])
                    self.broker.on_tick(user_id=uid, runner=r, price=price, at=as_of)

                    # Re-sync ORM after broker activity
                    try:
                        db.db.expire_all()
                    except Exception:
                        pass

                    # Refresh position AFTER broker.on_tick
                    try:
                        pos: Optional[OpenPosition] = (
                            db.db.query(OpenPosition)
                            .filter(OpenPosition.runner_id == r.id)
                            .first()
                        )
                    except Exception:
                        log.exception("Failed to refresh position for runner %s", r.id)
                        pos = None

                    # NEW: bar-advance guard (prevents same-bar flip-flops)
                    bar_key = (r.id, tf)
                    prev_bar_ts = self._last_bar_ts.get(bar_key)
                    bar_advanced = (prev_bar_ts is None) or (last_ts is not None and last_ts > prev_bar_ts)

                    if not bar_advanced and self._require_bar_advance:
                        exec_buffer.append({
                            "runner_id": r.id,
                            "user_id": uid,
                            "symbol": r.stock,
                            "strategy": r.strategy,
                            "status": "completed",
                            "reason": "skipped-same-bar",
                            "details": (None if self._thin_no_action_details else json.dumps({
                                "message": "bar has not advanced; ignoring strategy signals this tick",
                                "tf_min": tf,
                                "last_bar_ts": (last_ts.isoformat() if last_ts else None),
                                "prev_bar_ts": (prev_bar_ts.isoformat() if prev_bar_ts else None),
                                "as_of": as_of.isoformat(),
                            }, ensure_ascii=False)),
                            "execution_time": as_of,
                            "cycle_seq": seq,
                        })
                        if self._log_no_action:
                            log.debug("NO_ACTION %s tf=%dm — same bar (last=%s, prev=%s).",
                                      r.stock, tf,
                                      (last_ts.isoformat() if last_ts else "None"),
                                      (prev_bar_ts.isoformat() if prev_bar_ts else "None"))
                        stats["no_action"] += 1
                        stats["processed"] += 1
                        continue

                    ctx = _RunnerCtx(runner=r, position=pos, price=price, candles=candles)
                    decision = self._decide(ctx)
                    action = (decision.get("action") or "NO_ACTION").upper()
                    explanation = decision.get("explanation")
                    checks = decision.get("checks")

                    details_payload = {
                        "price": round(ctx.price, 6),
                        "position_open": bool(ctx.position is not None),
                        "timeframe_min": tf,
                        "stale": False,
                        "last_ts": last_ts.isoformat() if last_ts else None,
                        "decision": {k: v for k, v in decision.items() if k not in {"action"}},
                        "checks": checks,
                    }
                    details_json = json.dumps(details_payload, ensure_ascii=False)

                    # BUY (only if no position and after bar advance)
                    if action == "BUY" and ctx.position is None:
                        qty = int(decision.get("quantity") or 0)
                        if qty <= 0:
                            qty = self._qty_from_budget(db, r, ctx.price)
                        if qty <= 0:
                            msg = {"reason": "qty=0", "explanation": explanation or "insufficient budget"}
                            exec_buffer.append({
                                "runner_id": r.id,
                                "user_id": uid,
                                "symbol": r.stock,
                                "strategy": r.strategy,
                                "status": "skipped-no-budget",
                                "reason": "qty=0",
                                "details": (None if self._thin_no_action_details else json.dumps(msg, ensure_ascii=False)),
                                "execution_time": as_of,
                                "cycle_seq": seq,
                            })
                            if self._log_no_action:
                                log.debug("NO_BUY %s tf=%dm — qty=0 | %s",
                                          r.stock, tf, (explanation or "").replace("\n", " | "))
                            stats["skipped_no_budget"] += 1
                            stats["processed"] += 1
                            if last_ts is not None:
                                self._last_bar_ts[bar_key] = last_ts
                            continue

                        ok: bool = False
                        try:
                            ok = bool(self.broker.buy(
                                user_id=uid,
                                runner=r,
                                symbol=r.stock,
                                price=ctx.price,
                                quantity=qty,
                                decision=decision,
                                at=as_of,
                            ))
                        except Exception:
                            ok = False
                            log.exception("Broker BUY failed for %s", r.stock)

                        if not ok:
                            exec_buffer.append({
                                "runner_id": r.id,
                                "user_id": uid,
                                "symbol": r.stock,
                                "strategy": r.strategy,
                                "status": "skipped-no-budget",
                                "reason": "broker_rejected_buy",
                                "details": (None if self._thin_no_action_details else details_json),
                                "execution_time": as_of,
                                "cycle_seq": seq,
                            })
                            if self._log_no_action:
                                log.debug("NO_BUY %s tf=%dm — broker rejected BUY (likely cash guard).",
                                          r.stock, tf)
                            stats["skipped_no_budget"] += 1
                            stats["processed"] += 1
                            if last_ts is not None:
                                self._last_bar_ts[bar_key] = last_ts
                            continue

                        # Arm trailing stop once at BUY (broker-managed)
                        try:
                            params = getattr(r, "parameters", {}) or {}
                            trail_pct = float(params.get("trailing_stop_percent", 0.0) or 0.0)
                        except Exception:
                            trail_pct = 0.0
                        if trail_pct > 0.0:
                            self.broker.arm_trailing_stop_once(
                                user_id=uid,
                                runner=r,
                                entry_price=ctx.price,
                                trail_pct=trail_pct,
                                at=as_of,
                            )

                        exec_buffer.append({
                            "runner_id": r.id,
                            "user_id": uid,
                            "symbol": r.stock,
                            "strategy": r.strategy,
                            "status": "completed",
                            "reason": "buy",
                            "details": details_json,
                            "execution_time": as_of,
                            "cycle_seq": seq,
                        })
                        log.debug("BUY %s qty=%d @ %.4f tf=%dm | %s",
                                  r.stock, qty, ctx.price, tf, (explanation or "").replace("\n", " | "))
                        stats["buys"] += 1
                        stats["processed"] += 1
                        if last_ts is not None:
                            self._last_bar_ts[bar_key] = last_ts
                        continue

                    # SELL (strategy-driven; only after bar advance)
                    if action == "SELL" and ctx.position is not None:
                        ok = False
                        try:
                            reason = str(decision.get("reason") or decision.get("explanation") or "strategy_sell")
                            ok = self.broker.sell_all(
                                user_id=uid,
                                runner=r,
                                symbol=r.stock,
                                price=ctx.price,
                                decision=decision,
                                at=as_of,
                                reason_override=reason,
                            )
                        except Exception:
                            ok = False
                            log.exception("Broker SELL failed for %s", r.stock)

                        try:
                            db.db.expire_all()
                        except Exception:
                            pass

                        exec_buffer.append({
                            "runner_id": r.id,
                            "user_id": uid,
                            "symbol": r.stock,
                            "strategy": r.strategy,
                            "status": "completed" if ok else "error",
                            "reason": "sell" if ok else "broker_sell_failed",
                            "details": details_json,
                            "execution_time": as_of,
                            "cycle_seq": seq,
                        })
                        if ok:
                            log.debug("SELL %s @ %.4f tf=%dm | %s",
                                      r.stock, ctx.price, tf, (explanation or "").replace("\n", " | "))
                            stats["sells"] += 1
                        else:
                            stats["errors"] += 1
                        stats["processed"] += 1
                        if last_ts is not None:
                            self._last_bar_ts[bar_key] = last_ts
                        continue

                    # NO_ACTION
                    exec_buffer.append({
                        "runner_id": r.id,
                        "user_id": uid,
                        "symbol": r.stock,
                        "strategy": r.strategy,
                        "status": "completed",
                        "reason": str(decision.get("reason") or "no_action"),
                        "details": (None if self._thin_no_action_details else details_json),
                        "execution_time": as_of,
                        "cycle_seq": seq,
                    })
                    if self._log_no_action:
                        if explanation:
                            log.debug("NO_ACTION %s tf=%dm — %s", r.stock, tf, explanation.replace("\n", " | "))
                        else:
                            log.debug("NO_ACTION %s tf=%dm", r.stock, tf)
                    stats["no_action"] += 1
                    stats["processed"] += 1
                    if last_ts is not None:
                        self._last_bar_ts[bar_key] = last_ts

                except Exception:
                    try:
                        label = getattr(r, "name", None) or f"#{getattr(r, 'id', 'unknown')}"
                    except Exception:
                        label = "unknown"
                    log.exception("Runner %s tick failed", label)
                    try:
                        exec_buffer.append({
                            "runner_id": int(getattr(r, "id", 0) or 0),
                            "user_id": uid,
                            "symbol": (getattr(r, "stock", "") or "").upper() or "UNKNOWN",
                            "strategy": (getattr(r, "strategy", "") or "unknown"),
                            "status": "error",
                            "reason": "exception",
                            "details": "see logs",
                            "execution_time": as_of,
                            "cycle_seq": seq,
                        })
                    except Exception:
                        pass
                    stats["errors"] += 1
                    stats["processed"] += 1

            # Batch persist executions
            try:
                if exec_buffer:
                    db.bulk_record_runner_executions(exec_buffer)
            except Exception:
                log.exception("Bulk insert of runner executions failed")

            # Account mark-to-market (mock)
            try:
                self.broker.mark_to_market_all(user_id=uid, at=as_of)
            except Exception:
                log.exception("Mark-to-market after tick failed")

        log.debug(
            "tick@%s processed=%d buys=%d sells=%d no_action=%d skipped_no_data=%d skipped_no_budget=%d errors=%d",
            as_of.isoformat(),
            stats["processed"],
            stats["buys"],
            stats["sells"],
            stats["no_action"],
            stats["skipped_no_data"],
            stats["skipped_no_budget"],
            stats["errors"],
        )
        return stats



===== backend/logger_config.py =====

DESCRIPTION: Source file

import logging
import os
from logging.handlers import RotatingFileHandler
from pathlib import Path

LOG_DIR = Path(os.getenv("LOG_DIR", "/root/projects/SelfTrading Analytics/logs"))
LOG_DIR.mkdir(parents=True, exist_ok=True)


def _mk_handler(filename: str, level: int) -> RotatingFileHandler:
    fh = RotatingFileHandler(
        LOG_DIR / filename,
        maxBytes=int(os.getenv("LOG_MAX_BYTES", str(10 * 1024 * 1024))),
        backupCount=int(os.getenv("LOG_BACKUP_COUNT", "5")),
        encoding="utf-8",
    )
    # NOTE: Use %(asctime)s and %(msecs)03d — strftime-based %f is not supported here.
    fmt = logging.Formatter(
        fmt="[%(asctime)s,%(msecs)03d] %(levelname)-7s %(name)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    fh.setFormatter(fmt)
    fh.setLevel(level)
    return fh


def setup_logging() -> None:
    root_level = getattr(logging, os.getenv("LOG_LEVEL", "DEBUG").upper(), logging.DEBUG)
    logging.basicConfig(level=root_level)

    # Core logs
    loggers = {
        "AnalyticsScheduler": ("sim_scheduler.log", logging.DEBUG),
        "runner-service": ("runner-service.log", logging.DEBUG),
        "mock-broker": (
            "mock-broker.log",
            getattr(logging, os.getenv("LOG_MOCK_BROKER_LEVEL", "INFO").upper(), logging.INFO),
        ),
        "grok-4-strategy": ("grok-4-strategy.log", logging.DEBUG),
        "chatgpt-5-strategy": ("chatgpt-5-strategy.log", logging.INFO),
        "api-gateway": ("api_gateway.log", logging.INFO),
        "errors_warnings": ("errors_warnings.log", logging.WARNING),
        "app": ("app.log", logging.INFO),
        "trades": ("trades.log", logging.INFO),

        # Data / session clock visibility
        "market-data-manager": ("market_data_manager.log", logging.INFO),

        # NEW: you already mirrored runner execution lines to a dedicated logger — give it a sink.
        "runner-executions": ("runner_executions.log", logging.INFO),
    }

    for name, (file, level) in loggers.items():
        lg = logging.getLogger(name)
        lg.setLevel(level)
        if not any(
            isinstance(h, RotatingFileHandler) and getattr(h, "baseFilename", "").endswith(file)
            for h in lg.handlers
        ):
            lg.addHandler(_mk_handler(file, level))

    # Ensure all WARNING+ logs from any logger also go to errors_warnings.log
    root_logger = logging.getLogger()
    root_logger.setLevel(root_level)
    ew_file = "errors_warnings.log"
    if not any(
        isinstance(h, RotatingFileHandler) and getattr(h, "baseFilename", "").endswith(ew_file)
        for h in root_logger.handlers
    ):
        root_logger.addHandler(_mk_handler(ew_file, logging.WARNING))

    # Quiet noisy libs unless explicitly raised
    logging.getLogger("uvicorn.error").setLevel(logging.INFO)
    logging.getLogger("asyncio").setLevel(logging.WARNING)


===== backend/ib_manager/market_data_manager.py =====

DESCRIPTION: Source file

from __future__ import annotations

import os
import logging
from datetime import datetime, timezone, timedelta, time, date
from typing import List, Dict, Any, Tuple, Optional, Iterable

from sqlalchemy import select, func

from database.db_core import engine
from database.models import HistoricalMinuteBar, HistoricalDailyBar

try:
    # Python 3.9+
    from zoneinfo import ZoneInfo  # type: ignore
    _NY = ZoneInfo("America/New_York")
except Exception:
    _NY = None  # Fallback handled below

log = logging.getLogger("market-data-manager")


def _ensure_utc(dt: datetime) -> datetime:
    return dt if dt.tzinfo else dt.replace(tzinfo=timezone.utc)


def _et_bounds_for_date(et_day: date) -> Tuple[datetime, datetime]:
    """
    Return NY session bounds [open, close] as UTC datetimes for a given ET calendar date.
    """
    if _NY is None:
        # Fallback: treat ET == UTC (won't shift with DST; not ideal but safe)
        open_utc = datetime(et_day.year, et_day.month, et_day.day, 9, 30, tzinfo=timezone.utc)
        close_utc = datetime(et_day.year, et_day.month, et_day.day, 16, 0, tzinfo=timezone.utc)
        return open_utc, close_utc

    open_et = datetime(et_day.year, et_day.month, et_day.day, 9, 30, tzinfo=_NY)
    close_et = datetime(et_day.year, et_day.month, et_day.day, 16, 0, tzinfo=_NY)
    return open_et.astimezone(timezone.utc), close_et.astimezone(timezone.utc)


def _is_weekday(et_dt: datetime) -> bool:
    # Monday=0 .. Sunday=6
    return et_dt.weekday() < 5


def _is_regular_market_minute(ts_utc: datetime) -> bool:
    """
    True iff ts_utc lies inside a regular-hours NYSE minute (Mon-Fri, 09:30<=t<=16:00 ET).
    Holidays are not detected here; we filter those by data presence elsewhere.
    """
    ts_utc = _ensure_utc(ts_utc)
    if _NY is None:
        # Conservative fallback: accept only weekday UTC 13:30..20:00 (roughly 09:30..16:00 ET, no DST)
        t = ts_utc.time()
        return ts_utc.weekday() < 5 and time(13, 30) <= t <= time(20, 0)

    et = ts_utc.astimezone(_NY)
    if not _is_weekday(et):
        return False
    t = et.time()
    return time(9, 30) <= t <= time(16, 0)


class MarketDataManager:
    """
    Historical data access + small helpers.
    • RTH filtering for intraday.
    • Session-aware 'next tick' that respects holidays/DST using actual bars.
    • Resilient session clock: auto-pick a valid symbol and global fallback.
    """

    def __init__(self) -> None:
        self._last_session: Tuple[Optional[datetime], str] = (None, "regular-hours")
        self._clock_symbol = os.getenv("SIM_REFERENCE_CLOCK_SYMBOL", "SPY").upper()

    # ─────────────────────────── diagnostics / coverage ───────────────────────────

    def has_minute_bars(self, symbol: str, interval_min: int) -> bool:
        """Return True if any minute bars exist for symbol/interval."""
        symbol = (symbol or "").upper()
        with engine.connect() as conn:
            ts = conn.execute(
                select(func.min(HistoricalMinuteBar.ts))
                .where(HistoricalMinuteBar.symbol == symbol)
                .where(HistoricalMinuteBar.interval_min == int(interval_min))
            ).scalar()
            return ts is not None

    def has_daily_bars(self, symbol: str) -> bool:
        """Return True if any daily bars exist for symbol."""
        symbol = (symbol or "").upper()
        with engine.connect() as conn:
            dt = conn.execute(
                select(func.min(HistoricalDailyBar.date))
                .where(HistoricalDailyBar.symbol == symbol)
            ).scalar()
            return dt is not None

    def pick_reference_symbol(
        self,
        interval_min: int = 5,
        prefer: Optional[Iterable[str]] = None,
    ) -> Optional[str]:
        """
        Choose a good clock symbol for the given interval:
          1) first available from 'prefer' list
          2) the symbol with the most rows at that interval
        """
        prefer_list = [s.strip().upper() for s in (prefer or
                        os.getenv("SIM_REFERENCE_CANDIDATES", "SPY,QQQ,AAPL,MSFT,TSLA,AMD,NVDA,GOOGL,AMZN").split(","))]
        for s in prefer_list:
            try:
                if s and self.has_minute_bars(s, interval_min):
                    return s
            except Exception:
                continue

        # Fallback: most-populated symbol at this interval
        with engine.connect() as conn:
            rows = conn.execute(
                select(HistoricalMinuteBar.symbol, func.count().label("n"))
                .where(HistoricalMinuteBar.interval_min == int(interval_min))
                .group_by(HistoricalMinuteBar.symbol)
                .order_by(func.count().desc())
                .limit(1)
            ).all()
            if rows:
                return rows[0]._mapping["symbol"]
        return None

    # ─────────────────────────── data access (single symbol) ───────────────────────────

    def get_candles_until(
        self,
        symbol: str,
        interval_min: int,
        as_of: datetime,
        lookback: int = 250,
        *,
        regular_hours_only: bool = True,
    ) -> List[Dict[str, Any]]:
        symbol = symbol.upper()
        as_of = _ensure_utc(as_of)

        with engine.connect() as conn:
            if int(interval_min) == 1440:
                stmt = (
                    select(
                        HistoricalDailyBar.date,
                        HistoricalDailyBar.open,
                        HistoricalDailyBar.high,
                        HistoricalDailyBar.low,
                        HistoricalDailyBar.close,
                        HistoricalDailyBar.volume,
                    )
                    .where(HistoricalDailyBar.symbol == symbol)
                    .where(HistoricalDailyBar.date <= as_of)
                    .order_by(HistoricalDailyBar.date.desc())
                    .limit(lookback)
                )
                rows = conn.execute(stmt).all()
                rows.reverse()
                out: List[Dict[str, Any]] = []
                for row in rows:
                    m = row._mapping
                    out.append(
                        {
                            "ts": m["date"],
                            "open": m["open"],
                            "high": m["high"],
                            "low": m["low"],
                            "close": m["close"],
                            "volume": m["volume"],
                        }
                    )
                return out

            # minute bars
            raw_limit = lookback * (3 if regular_hours_only else 1)
            stmt = (
                select(
                    HistoricalMinuteBar.ts,
                    HistoricalMinuteBar.open,
                    HistoricalMinuteBar.high,
                    HistoricalMinuteBar.low,
                    HistoricalMinuteBar.close,
                    HistoricalMinuteBar.volume,
                )
                .where(HistoricalMinuteBar.symbol == symbol)
                .where(HistoricalMinuteBar.interval_min == int(interval_min))
                .where(HistoricalMinuteBar.ts <= as_of)
                .order_by(HistoricalMinuteBar.ts.desc())
                .limit(raw_limit)
            )
            rows = conn.execute(stmt).all()
            rows.reverse()

            out: List[Dict[str, Any]] = []
            for row in rows:
                m = row._mapping
                ts = m["ts"]
                ts = ts if getattr(ts, "tzinfo", None) else ts.replace(tzinfo=timezone.utc)
                if (not regular_hours_only) or _is_regular_market_minute(ts):
                    out.append(
                        {
                            "ts": ts,
                            "open": m["open"],
                            "high": m["high"],
                            "low": m["low"],
                            "close": m["close"],
                            "volume": m["volume"],
                        }
                    )
            if regular_hours_only and len(out) > lookback:
                out = out[-lookback:]
            return out

    # ─────────────────────────── data access (BULK) ───────────────────────────

    def get_candles_bulk_until(
        self,
        symbols: List[str],
        interval_min: int,
        as_of: datetime,
        lookback: int = 250,
        *,
        regular_hours_only: bool = True,
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Return up to `lookback` bars PER symbol for many symbols in a single query.
        Uses row_number() OVER (PARTITION BY symbol ORDER BY ts/date DESC).

        Output: { "AAPL": [ {ts, open, high, low, close, volume}, ... ], ... }
        Bars are oldest→newest for each symbol.
        """
        if not symbols:
            return {}
        syms = [s.upper() for s in symbols]
        as_of = _ensure_utc(as_of)

        out: Dict[str, List[Dict[str, Any]]] = {s: [] for s in syms}

        with engine.connect() as conn:
            if int(interval_min) == 1440:
                rn = func.row_number().over(
                    partition_by=HistoricalDailyBar.symbol,
                    order_by=HistoricalDailyBar.date.desc(),
                ).label("rn")
                base = (
                    select(
                        HistoricalDailyBar.symbol.label("symbol"),
                        HistoricalDailyBar.date.label("ts"),
                        HistoricalDailyBar.open.label("open"),
                        HistoricalDailyBar.high.label("high"),
                        HistoricalDailyBar.low.label("low"),
                        HistoricalDailyBar.close.label("close"),
                        HistoricalDailyBar.volume.label("volume"),
                        rn,
                    )
                    .where(HistoricalDailyBar.symbol.in_(syms))
                    .where(HistoricalDailyBar.date <= as_of)
                ).subquery("d")
                stmt = (
                    select(
                        base.c.symbol,
                        base.c.ts,
                        base.c.open,
                        base.c.high,
                        base.c.low,
                        base.c.close,
                        base.c.volume,
                    )
                    .where(base.c.rn <= int(lookback))
                    .order_by(base.c.symbol.asc(), base.c.ts.asc())
                )
                rows = conn.execute(stmt).all()
                for row in rows:
                    m = row._mapping
                    out[m["symbol"]].append(
                        {
                            "ts": m["ts"],
                            "open": m["open"],
                            "high": m["high"],
                            "low": m["low"],
                            "close": m["close"],
                            "volume": m["volume"],
                        }
                    )
            else:
                rn = func.row_number().over(
                    partition_by=HistoricalMinuteBar.symbol,
                    order_by=HistoricalMinuteBar.ts.desc(),
                ).label("rn")
                raw_limit = lookback * (3 if regular_hours_only else 1)
                base = (
                    select(
                        HistoricalMinuteBar.symbol.label("symbol"),
                        HistoricalMinuteBar.ts.label("ts"),
                        HistoricalMinuteBar.open.label("open"),
                        HistoricalMinuteBar.high.label("high"),
                        HistoricalMinuteBar.low.label("low"),
                        HistoricalMinuteBar.close.label("close"),
                        HistoricalMinuteBar.volume.label("volume"),
                        rn,
                    )
                    .where(HistoricalMinuteBar.symbol.in_(syms))
                    .where(HistoricalMinuteBar.interval_min == int(interval_min))
                    .where(HistoricalMinuteBar.ts <= as_of)
                ).subquery("m")
                stmt = (
                    select(
                        base.c.symbol,
                        base.c.ts,
                        base.c.open,
                        base.c.high,
                        base.c.low,
                        base.c.close,
                        base.c.volume,
                    )
                    .where(base.c.rn <= int(raw_limit))
                    .order_by(base.c.symbol.asc(), base.c.ts.asc())
                )
                rows = conn.execute(stmt).all()
                for row in rows:
                    m = row._mapping
                    ts = m["ts"]
                    ts = ts if getattr(ts, "tzinfo", None) else ts.replace(tzinfo=timezone.utc)
                    if (not regular_hours_only) or _is_regular_market_minute(ts):
                        out[m["symbol"]].append(
                            {
                                "ts": ts,
                                "open": m["open"],
                                "high": m["high"],
                                "low": m["low"],
                                "close": m["close"],
                                "volume": m["volume"],
                            }
                        )

                # Trim to lookback per symbol if we over-fetched for filtering
                if regular_hours_only:
                    for s in syms:
                        if len(out[s]) > lookback:
                            out[s] = out[s][-lookback:]

        return out

    # ─────────────────────────── session-aware tick helpers ───────────────────────────

    def get_next_session_ts(
        self,
        as_of: datetime,
        interval_min: int = 5,
        *,
        reference_symbol: Optional[str] = None,
    ) -> Optional[datetime]:
        """
        Return the next candle timestamp (>= as_of+ε) that lies within a regular-hours
        NY session for `reference_symbol` at `interval_min`.

        If `reference_symbol` is absent or has no coverage for the session/day,
        we automatically fallback to the earliest bar **from ANY symbol** within
        the same RTH window. Holidays & DST still respected because we only return
        timestamps that actually exist in the database.
        """
        as_of = _ensure_utc(as_of)
        clock_sym = ((reference_symbol or self._clock_symbol or "SPY") or "").upper()

        # Try up to 400 consecutive ET days forward (safety bound)
        start_et = as_of.astimezone(_NY) if _NY else as_of
        day = start_et.date()
        epsilon = timedelta(seconds=1)

        with engine.connect() as conn:
            for _ in range(400):
                open_utc, close_utc = _et_bounds_for_date(day)

                if as_of >= close_utc:
                    # Past today's close — move to next calendar day
                    day = (datetime.combine(day, time(0, 0)) + timedelta(days=1)).date()
                    continue

                search_from = max(as_of + epsilon, open_utc)
                if search_from > close_utc:
                    # Before open or after close — skip to next day
                    day = (datetime.combine(day, time(0, 0)) + timedelta(days=1)).date()
                    continue

                next_ts = None

                # 1) Preferred clock symbol
                if clock_sym:
                    next_ts = conn.execute(
                        select(func.min(HistoricalMinuteBar.ts))
                        .where(HistoricalMinuteBar.symbol == clock_sym)
                        .where(HistoricalMinuteBar.interval_min == int(interval_min))
                        .where(HistoricalMinuteBar.ts >= search_from)
                        .where(HistoricalMinuteBar.ts <= close_utc)
                    ).scalar()

                # 2) Fallback: any symbol at this interval in the same RTH window
                if next_ts is None:
                    next_ts = conn.execute(
                        select(func.min(HistoricalMinuteBar.ts))
                        .where(HistoricalMinuteBar.interval_min == int(interval_min))
                        .where(HistoricalMinuteBar.ts >= search_from)
                        .where(HistoricalMinuteBar.ts <= close_utc)
                    ).scalar()
                    if next_ts:
                        log.debug(
                            "next_session_ts: using GLOBAL fallback at %s (tf=%dm) because clock '%s' had no bar.",
                            next_ts, interval_min, (clock_sym or "<none>")
                        )

                if next_ts:
                    ts = next_ts if getattr(next_ts, "tzinfo", None) else next_ts.replace(tzinfo=timezone.utc)
                    log.debug(
                        "next_session_ts: as_of=%s -> %s (clock=%s, tf=%dm)",
                        as_of.isoformat(),
                        ts.isoformat(),
                        (clock_sym or "<auto>"),
                        interval_min,
                    )
                    return ts

                # No bars in this session → likely holiday or the dataset lacks coverage for this day.
                day = (datetime.combine(day, time(0, 0)) + timedelta(days=1)).date()

        log.warning(
            "next_session_ts: No further bars found after %s (clock=%s, tf=%dm)",
            as_of.isoformat(),
            (clock_sym or "<auto>"),
            interval_min,
        )
        return None

    def get_next_session_ts_global(self, as_of: datetime, interval_min: int = 5) -> Optional[datetime]:
        """Convenience wrapper for callers who explicitly want the global fallback."""
        return self.get_next_session_ts(as_of, interval_min=interval_min, reference_symbol=None)

    # ─────────────────────────── indicators ───────────────────────────

    @staticmethod
    def calculate_sma(candles: List[Dict[str, Any]], period: int) -> float:
        if len(candles) < period:
            return float("nan")
        closes = [float(c["close"]) for c in candles[-period:]]
        return sum(closes) / len(closes)

    @staticmethod
    def calculate_ema(candles: List[Dict[str, Any]], period: int) -> float:
        if len(candles) < period:
            return float("nan")
        k = 2.0 / (period + 1.0)
        ema = float(candles[-period]["close"])
        for c in candles[-period + 1:]:
            ema = c["close"] * k + ema * (1.0 - k)
        return float(ema)

    @staticmethod
    def calculate_rsi(candles: List[Dict[str, Any]], period: int = 14) -> float:
        if len(candles) < period + 1:
            return float("nan")
        gains = 0.0
        losses = 0.0
        closes = [float(c["close"]) for c in candles[-(period + 1):]]
        for i in range(1, len(closes)):
            delta = closes[i] - closes[i - 1]
            if delta >= 0:
                gains += delta
            else:
                losses -= delta
        avg_gain = gains / period
        avg_loss = losses / period
        if avg_loss == 0:
            return 100.0
        rs = avg_gain / avg_loss
        return 100.0 - (100.0 / (1.0 + rs))

    @staticmethod
    def calculate_atr(candles: List[Dict[str, Any]], period: int = 14) -> float:
        if len(candles) < period + 1:
            return float("nan")
        trs = []
        for i in range(1, period + 1):
            h = float(candles[-i]["high"])
            l = float(candles[-i]["low"])
            pc = float(candles[-i - 1]["close"])
            trs.append(max(h - l, abs(h - pc), abs(l - pc)))
        return sum(trs) / len(trs)

    @staticmethod
    def average_volume(candles: List[Dict[str, Any]], period: int) -> float:
        if len(candles) < period:
            return 0.0
        vols = [float(c["volume"]) for c in candles[-period:]]
        return sum(vols) / float(period)

    @staticmethod
    def donchian_channel(
        candles: List[Dict[str, Any]], lookback: int
    ) -> Tuple[Optional[float], Optional[float]]:
        if len(candles) < lookback:
            return (None, None)
        window = candles[-lookback:]
        highs = [float(c["high"]) for c in window]
        lows = [float(c["low"]) for c in window]
        return (max(highs), min(lows))

    # ─────────────────────────── mark-to-market helpers ───────────────────────────

    def get_last_close_for_symbols(
        self,
        symbols: List[str],
        minutes: int,
        as_of: datetime,
        *,
        regular_hours_only: bool = True,
    ) -> Dict[str, float]:
        """
        Return a mapping {symbol: last_close_price} using the most recent candle
        at or before `as_of` for the requested timeframe.

        • minutes >= 1440 → use daily bars (<= as_of date)
        • minutes < 1440  → use minute bars with interval_min=minutes
        • When regular_hours_only=True for intraday, only RTH minutes are considered.
        • Symbols with no price are omitted from the result.
        """
        as_of = _ensure_utc(as_of)
        if not symbols:
            return {}
        syms = [s.upper() for s in symbols]

        out: Dict[str, float] = {}

        with engine.connect() as conn:
            # DAILY
            if int(minutes) >= 1440:
                rn = func.row_number().over(
                    partition_by=HistoricalDailyBar.symbol,
                    order_by=HistoricalDailyBar.date.desc(),
                ).label("rn")
                base = (
                    select(
                        HistoricalDailyBar.symbol.label("symbol"),
                        HistoricalDailyBar.date.label("ts"),
                        HistoricalDailyBar.close.label("close"),
                        rn,
                    )
                    .where(HistoricalDailyBar.symbol.in_(syms))
                    .where(HistoricalDailyBar.date <= as_of)
                ).subquery("d_last")
                stmt = (
                    select(base.c.symbol, base.c.close)
                    .where(base.c.rn == 1)
                )
                for row in conn.execute(stmt).all():
                    m = row._mapping
                    try:
                        out[m["symbol"]] = float(m["close"])
                    except Exception:
                        continue
                return out

            # MINUTE
            rn = func.row_number().over(
                partition_by=HistoricalMinuteBar.symbol,
                order_by=HistoricalMinuteBar.ts.desc(),
            ).label("rn")

            base = (
                select(
                    HistoricalMinuteBar.symbol.label("symbol"),
                    HistoricalMinuteBar.ts.label("ts"),
                    HistoricalMinuteBar.close.label("close"),
                    rn,
                )
                .where(HistoricalMinuteBar.symbol.in_(syms))
                .where(HistoricalMinuteBar.interval_min == int(minutes))
                .where(HistoricalMinuteBar.ts <= as_of)
            ).subquery("m_last")

            stmt = select(base.c.symbol, base.c.ts, base.c.close).where(base.c.rn <= 3)
            rows = conn.execute(stmt).all()

            # Filter for RTH if requested and pick the first valid
            grouped: Dict[str, List[Tuple[datetime, float]]] = {}
            for row in rows:
                m = row._mapping
                ts = m["ts"]
                ts = ts if getattr(ts, "tzinfo", None) else ts.replace(tzinfo=timezone.utc)
                grouped.setdefault(m["symbol"], []).append((ts, float(m["close"])))

            for s, items in grouped.items():
                if regular_hours_only:
                    items = [(ts, px) for (ts, px) in items if _is_regular_market_minute(ts)]
                if not items:
                    continue
                items.sort(key=lambda x: x[0], reverse=True)
                out[s] = items[0][1]

        return out


===== backend/trades_logger.py =====

DESCRIPTION: Source file

from __future__ import annotations
import logging
from datetime import datetime, timezone

log = logging.getLogger("trades")

def _ts(dt) -> str:
    if isinstance(dt, datetime):
        return (dt if dt.tzinfo else dt.replace(tzinfo=timezone.utc)).isoformat()
    return str(dt)

def log_buy(*, user_id: int, runner_id: int, symbol: str, qty: float, price: float, as_of, reason: str = "") -> None:
    """
    Write a BUY fill line to trades.log
    """
    log.info(
        "BUY user=%s runner=%s symbol=%s qty=%.4f fill=%.4f as_of=%s%s",
        user_id,
        runner_id,
        symbol,
        qty,
        price,
        _ts(as_of),
        f" reason={reason}" if reason else "",
    )

def log_sell(*, user_id: int, runner_id: int, symbol: str, qty: float, avg_price: float, price: float, as_of, reason: str = "") -> None:
    """
    Write a SELL fill line to trades.log including P&L and P&L%.
    """
    pnl = (price - avg_price) * qty
    pnl_pct = 0.0 if avg_price == 0 else ((price / avg_price) - 1.0) * 100.0
    log.info(
        "SELL user=%s runner=%s symbol=%s qty=%.4f fill=%.4f avg=%.4f pnl=%.2f pnl_pct=%.2f%% as_of=%s%s",
        user_id,
        runner_id,
        symbol,
        qty,
        price,
        avg_price,
        pnl,
        pnl_pct,
        _ts(as_of),
        f" reason={reason}" if reason else "",
    )


===== backend/database/db_core.py =====

DESCRIPTION: Source file

from __future__ import annotations

import os
import time
import socket
import logging
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker, scoped_session, DeclarativeBase

log = logging.getLogger("database.db_core")


def _first_resolvable_host(candidates: list[str]) -> str:
    """Return the first host that resolves via DNS; fall back to the first item."""
    for h in candidates:
        try:
            socket.getaddrinfo(h, None)
            return h
        except Exception:
            continue
    return candidates[0]


def _build_url() -> str:
    user = os.getenv("DB_USER") or os.getenv("POSTGRES_USER", "postgres")
    pwd = os.getenv("DB_PASSWORD") or os.getenv("POSTGRES_PASSWORD", "postgres")
    # Prefer explicit envs; otherwise probe a few common service names
    cand = []
    if os.getenv("POSTGRES_HOST"):
        cand.append(os.getenv("POSTGRES_HOST", ""))
    if os.getenv("DB_HOST"):
        cand.append(os.getenv("DB_HOST", ""))
    cand += ["db", "postgres", "localhost", "127.0.0.1"]
    host = _first_resolvable_host([h for h in cand if h])

    port = os.getenv("POSTGRES_PORT", "5432")
    db   = os.getenv("POSTGRES_DB", "selftrading_analytics_db")
    return f"postgresql://{user}:{pwd}@{host}:{port}/{db}"


class Base(DeclarativeBase):
    pass


# Accept common envs in this order: DATABASE_URL (12-factor), DATABASE_URL_DOCKER, or build.
DATABASE_URL = os.getenv("DATABASE_URL") or os.getenv("DATABASE_URL_DOCKER") or _build_url()

POOL_SIZE = int(os.getenv("DB_POOL_SIZE", "10"))
MAX_OVER = int(os.getenv("DB_MAX_OVERFLOW", "20"))
RECYCLE = int(os.getenv("DB_POOL_RECYCLE", "1800"))
TIMEOUT = int(os.getenv("DB_POOL_TIMEOUT", "30"))
CONNECT_TIMEOUT = int(os.getenv("DB_CONNECT_TIMEOUT", "5"))

engine = create_engine(
    DATABASE_URL,
    pool_size=POOL_SIZE,
    max_overflow=MAX_OVER,
    pool_recycle=RECYCLE,
    pool_pre_ping=True,
    connect_args={"connect_timeout": CONNECT_TIMEOUT},
)

SessionLocal = scoped_session(sessionmaker(bind=engine, autocommit=False, autoflush=False, expire_on_commit=False))


def wait_for_db_ready(max_wait_seconds: int | None = None) -> None:
    """
    Ping the DB until it responds or time elapses.
    max_wait_seconds: environment DB_CONNECT_MAX_WAIT (default 30) if None.
    """
    deadline = time.monotonic() + int(os.getenv("DB_CONNECT_MAX_WAIT", "30") if max_wait_seconds is None else max_wait_seconds)
    attempt = 0
    while True:
        attempt += 1
        try:
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            if attempt > 1:
                log.info("Database is ready (after %d attempts).", attempt)
            return
        except Exception as e:
            if time.monotonic() >= deadline:
                log.error("Database not ready after retries: %s", e)
                raise
            time.sleep(min(0.5 * attempt, 3.0))


===== backend/database/init_db.py =====

DESCRIPTION: Source file

from __future__ import annotations

import logging
from contextlib import suppress

from sqlalchemy import text
from database.db_core import engine

log = logging.getLogger("app")


def _exec(conn, sql: str, params: dict | None = None) -> None:
    """Execute a statement and swallow errors so migrations are idempotent."""
    with suppress(Exception):
        conn.execute(text(sql), params or {})


def _column_is_nullable(conn, table: str, column: str) -> bool | None:
    """
    Return True if column is nullable, False if not nullable, or None if unknown.
    Works on Postgres via information_schema.
    """
    sql = """
        SELECT is_nullable
          FROM information_schema.columns
         WHERE table_schema = current_schema()
           AND table_name = :t
           AND column_name = :c
        LIMIT 1
    """
    row = conn.execute(text(sql), {"t": table, "c": column}).fetchone()
    if not row:
        return None
    is_nullable = (row[0] or "").strip().upper()
    return is_nullable == "YES"


def _table_exists(conn, table: str) -> bool:
    sql = """
        SELECT 1
          FROM information_schema.tables
         WHERE table_schema = current_schema()
           AND table_name = :t
        LIMIT 1
    """
    return conn.execute(text(sql), {"t": table}).scalar() is not None


def _apply_light_migrations() -> None:
    """
    Lightweight, idempotent migrations. Safe to run at every process start.
    """
    try:
        with engine.begin() as conn:
            # ─────────────────────────────────────────────────────────────
            # orders.side (back-compat: 'action' relaxed to NULL)
            # ─────────────────────────────────────────────────────────────
            _exec(conn, "ALTER TABLE IF EXISTS orders ADD COLUMN IF NOT EXISTS side VARCHAR(8)")
            _exec(conn, "ALTER TABLE IF EXISTS orders ALTER COLUMN action DROP NOT NULL")

            # ─────────────────────────────────────────────────────────────
            # executed_trades: ensure essential columns exist
            # (buy_ts, sell_ts, prices, qty, pnl_*, strategy, timeframe)
            # Types chosen to be broadly compatible; adjust if your DDL differs.
            # ─────────────────────────────────────────────────────────────
            if _table_exists(conn, "executed_trades"):
                _exec(conn, "ALTER TABLE executed_trades ADD COLUMN IF NOT EXISTS user_id INTEGER")
                _exec(conn, "ALTER TABLE executed_trades ADD COLUMN IF NOT EXISTS runner_id INTEGER")
                _exec(conn, "ALTER TABLE executed_trades ADD COLUMN IF NOT EXISTS symbol VARCHAR(32)")
                _exec(conn, "ALTER TABLE executed_trades ADD COLUMN IF NOT EXISTS buy_ts TIMESTAMPTZ")
                _exec(conn, "ALTER TABLE executed_trades ADD COLUMN IF NOT EXISTS sell_ts TIMESTAMPTZ")
                _exec(conn, "ALTER TABLE executed_trades ADD COLUMN IF NOT EXISTS buy_price NUMERIC(18,6)")
                _exec(conn, "ALTER TABLE executed_trades ADD COLUMN IF NOT EXISTS sell_price NUMERIC(18,6)")
                _exec(conn, "ALTER TABLE executed_trades ADD COLUMN IF NOT EXISTS quantity NUMERIC(18,6)")
                _exec(conn, "ALTER TABLE executed_trades ADD COLUMN IF NOT EXISTS pnl_amount NUMERIC(18,6)")
                _exec(conn, "ALTER TABLE executed_trades ADD COLUMN IF NOT EXISTS pnl_percent NUMERIC(9,6)")
                _exec(conn, "ALTER TABLE executed_trades ADD COLUMN IF NOT EXISTS strategy VARCHAR(64)")
                _exec(conn, "ALTER TABLE executed_trades ADD COLUMN IF NOT EXISTS timeframe VARCHAR(16)")

                # ─────────────────────────────────────────────────────────
                # CRITICAL: relax NOT NULL on perm_id for simulation
                # Keep the column if present, just allow NULLs.
                # ─────────────────────────────────────────────────────────
                nullable = _column_is_nullable(conn, "executed_trades", "perm_id")
                if nullable is False:
                    _exec(conn, "ALTER TABLE executed_trades ALTER COLUMN perm_id DROP NOT NULL")
                    log.info("Light migrations: relaxed executed_trades.perm_id to NULL (sim-safe).")

                # Optional helpful indexes (idempotent CREATE INDEX IF NOT EXISTS)
                _exec(conn, "CREATE INDEX IF NOT EXISTS ix_executed_trades_user_runner ON executed_trades (user_id, runner_id)")
                _exec(conn, "CREATE INDEX IF NOT EXISTS ix_executed_trades_sell_ts ON executed_trades (sell_ts)")
            else:
                log.warning("Light migrations: executed_trades table not found; skipping column checks.")

        # Mirror the same lines that your logs already show for visibility
        log.info("Light migrations: ensured orders.side (VARCHAR(8)).")
        log.info("Light migrations: relaxed orders.action to NULL (back-compat with 'side').")
        log.info("Light migrations: ensured executed_trades columns (buy_ts, sell_ts, buy/sell_price, qty, pnl_*, strategy, timeframe).")

    except Exception:
        log.exception("Light migrations failed")


===== backend/database/models.py =====

DESCRIPTION: Source file

from __future__ import annotations

from datetime import datetime
from typing import Optional

from sqlalchemy import (
    String, Integer, Float, DateTime, ForeignKey, UniqueConstraint, Index, JSON, Text,
    BigInteger, Numeric
)
from sqlalchemy.orm import Mapped, mapped_column
from database.db_core import Base

# ───────── Users ─────────
class User(Base):
    __tablename__ = "users"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    username: Mapped[str] = mapped_column(String(50), unique=True, index=True, nullable=False)
    email: Mapped[str] = mapped_column(String(200), unique=True, nullable=False)
    password_hash: Mapped[str] = mapped_column(String(255), nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=datetime.utcnow)


# ───────── Runners ─────────
class Runner(Base):
    __tablename__ = "runners"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    user_id: Mapped[int] = mapped_column(ForeignKey("users.id", ondelete="CASCADE"), index=True)
    name: Mapped[str] = mapped_column(String(200))
    strategy: Mapped[str] = mapped_column(String(100), index=True)
    budget: Mapped[float] = mapped_column(Float, default=0.0)
    # NEW: track remaining budget; legacy DBs may already have NOT NULL constraint
    current_budget: Mapped[float] = mapped_column(Float, default=0.0)
    stock: Mapped[str] = mapped_column(String(20), index=True)
    time_frame: Mapped[int] = mapped_column(Integer, default=5)  # minutes; 1440 = 1d
    parameters: Mapped[dict] = mapped_column(JSON, default=dict)  # ← safe default
    time_range_from: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True)
    time_range_to:   Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True)
    exit_strategy: Mapped[str] = mapped_column(String(100), default="hold_forever")
    activation: Mapped[str] = mapped_column(String(20), default="active")
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=datetime.utcnow)

    __table_args__ = (Index("ix_runner_user_active", "user_id", "activation"),)


# ───────── Simulation state ─────────
class SimulationState(Base):
    __tablename__ = "simulation_state"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    user_id: Mapped[int] = mapped_column(ForeignKey("users.id", ondelete="CASCADE"), unique=True)
    is_running: Mapped[str] = mapped_column(String(5), default="false")  # "true"/"false"
    last_ts: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True)


# ───────── Historical data ─────────
class HistoricalDailyBar(Base):
    __tablename__ = "historical_daily_bars"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    symbol: Mapped[str] = mapped_column(String(20), index=True)
    date: Mapped[datetime] = mapped_column(DateTime(timezone=True), index=True)
    open: Mapped[float] = mapped_column(Float)
    high: Mapped[float] = mapped_column(Float)
    low:  Mapped[float] = mapped_column(Float)
    close: Mapped[float] = mapped_column(Float)
    volume: Mapped[int] = mapped_column(Integer)

    __table_args__ = (UniqueConstraint("symbol", "date", name="uq_daily_symbol_date"),)


class HistoricalMinuteBar(Base):
    __tablename__ = "historical_minute_bars"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    symbol: Mapped[str] = mapped_column(String(20), index=True)
    ts: Mapped[datetime] = mapped_column(DateTime(timezone=True), index=True)
    interval_min: Mapped[int] = mapped_column(Integer, index=True)  # 5 for 5m
    open: Mapped[float] = mapped_column(Float)
    high: Mapped[float] = mapped_column(Float)
    low:  Mapped[float] = mapped_column(Float)
    close: Mapped[float] = mapped_column(Float)
    volume: Mapped[int] = mapped_column(Integer)

    __table_args__ = (UniqueConstraint("symbol", "ts", "interval_min", name="uq_min_symbol_ts_interval"),)


# ───────── Mock Broker ─────────
class Account(Base):
    __tablename__ = "accounts"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    user_id: Mapped[int] = mapped_column(ForeignKey("users.id", ondelete="CASCADE"), index=True)
    name: Mapped[str] = mapped_column(String(50), default="mock")
    cash: Mapped[float] = mapped_column(Float, default=0.0)
    equity: Mapped[float] = mapped_column(Float, default=0.0)
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=datetime.utcnow)

    __table_args__ = (UniqueConstraint("user_id", "name", name="uq_account_user_name"),)


class OpenPosition(Base):
    __tablename__ = "open_positions"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    user_id: Mapped[int] = mapped_column(ForeignKey("users.id", ondelete="CASCADE"), index=True)
    runner_id: Mapped[int] = mapped_column(ForeignKey("runners.id", ondelete="CASCADE"), index=True, unique=True)
    symbol: Mapped[str] = mapped_column(String(20), index=True)

    # IMPORTANT: 'account' exists in the DB and is NOT NULL; reflect it here.
    account: Mapped[str] = mapped_column(String(50), default="mock", nullable=False, index=True)

    quantity: Mapped[int] = mapped_column(Integer)
    avg_price: Mapped[float] = mapped_column(Float)
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=datetime.utcnow)

    stop_price: Mapped[Optional[float]] = mapped_column(Float, nullable=True)
    trail_percent: Mapped[Optional[float]] = mapped_column(Float, nullable=True)
    highest_price: Mapped[Optional[float]] = mapped_column(Float, nullable=True)


class Order(Base):
    __tablename__ = "orders"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    user_id: Mapped[int] = mapped_column(Integer, index=True)
    runner_id: Mapped[int] = mapped_column(Integer, index=True)
    symbol: Mapped[str] = mapped_column(String(20), index=True)
    side: Mapped[str] = mapped_column(String(4))  # BUY/SELL
    order_type: Mapped[str] = mapped_column(String(20))  # MKT/LMT/etc
    quantity: Mapped[int] = mapped_column(Integer)
    limit_price: Mapped[Optional[float]] = mapped_column(Float, nullable=True)
    stop_price: Mapped[Optional[float]] = mapped_column(Float, nullable=True)
    status: Mapped[str] = mapped_column(String(20), default="filled")
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=datetime.utcnow)
    filled_at: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True)
    details: Mapped[Optional[str]] = mapped_column(Text, nullable=True)


class ExecutedTrade(Base):
    __tablename__ = "executed_trades"

    id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True)

    # BROKER permanent id (live). Optional in simulation.
    perm_id: Mapped[Optional[int]] = mapped_column(BigInteger, nullable=True)  # ← sim-safe

    # Ownership / attribution
    user_id: Mapped[Optional[int]] = mapped_column(Integer, index=True, nullable=True)
    runner_id: Mapped[Optional[int]] = mapped_column(Integer, index=True, nullable=True)

    # Instrument
    symbol: Mapped[Optional[str]] = mapped_column(String(32), nullable=True)

    # Timestamps
    buy_ts: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True)
    sell_ts: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True, index=True)

    # Prices & qty
    buy_price: Mapped[Optional[float]] = mapped_column(Numeric(18, 6), nullable=True)
    sell_price: Mapped[Optional[float]] = mapped_column(Numeric(18, 6), nullable=True)
    quantity: Mapped[Optional[float]] = mapped_column(Numeric(18, 6), nullable=True)

    # PnL (absolute and percent)
    pnl_amount: Mapped[Optional[float]] = mapped_column(Numeric(18, 6), nullable=True)
    pnl_percent: Mapped[Optional[float]] = mapped_column(Numeric(9, 6), nullable=True)

    # Strategy labeling
    strategy: Mapped[Optional[str]] = mapped_column(String(64), nullable=True)
    timeframe: Mapped[Optional[str]] = mapped_column(String(16), nullable=True)


class RunnerExecution(Base):
    __tablename__ = "runner_executions"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    runner_id: Mapped[int] = mapped_column(Integer, index=True)
    user_id: Mapped[int] = mapped_column(Integer, index=True)
    symbol: Mapped[str] = mapped_column(String(20))
    strategy: Mapped[str] = mapped_column(String(100))
    status: Mapped[str] = mapped_column(String(50))
    reason: Mapped[Optional[str]] = mapped_column(String(200), nullable=True)
    details: Mapped[Optional[str]] = mapped_column(Text, nullable=True)

    # NEW: simulation cycle sequence (e.g., epoch seconds of the tick)
    cycle_seq: Mapped[int] = mapped_column(Integer, index=True)

    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=datetime.utcnow)
    execution_time: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=datetime.utcnow, index=True)


class AnalyticsResult(Base):
    __tablename__ = "analytics_results"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    symbol: Mapped[str] = mapped_column(String(20), index=True)
    strategy: Mapped[str] = mapped_column(String(100), index=True)
    timeframe: Mapped[str] = mapped_column(String(10), index=True)

    start_ts: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True)
    end_ts:   Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True)

    final_pnl_amount: Mapped[float] = mapped_column(Float, default=0.0)
    final_pnl_percent: Mapped[float] = mapped_column(Float, default=0.0)
    trades_count: Mapped[int] = mapped_column(Integer, default=0)

    __table_args__ = (UniqueConstraint("symbol", "strategy", "timeframe", name="uq_result_key"),)


===== backend/database/db_manager.py =====

DESCRIPTION: Source file

from __future__ import annotations

import logging
import os
from contextlib import AbstractContextManager
from typing import Optional, Iterable, Dict, Any, List
from datetime import datetime, timezone

from sqlalchemy.orm import Session
from sqlalchemy import select, func

from database.db_core import SessionLocal
from database.models import (
    User,
    Runner,
    SimulationState,
    Account,
    OpenPosition,
    Order,
    ExecutedTrade,
    RunnerExecution,
    AnalyticsResult,
)

# password hashing for user bootstrap
try:
    from passlib.context import CryptContext  # type: ignore
    _pwd_ctx = CryptContext(schemes=["bcrypt"], deprecated="auto")
except Exception:
    _pwd_ctx = None  # pragma: no cover

log = logging.getLogger("database.db_manager")
_exec_log = logging.getLogger("runner-executions")


def _now_utc() -> datetime:
    return datetime.now(timezone.utc)


class DBManager(AbstractContextManager["DBManager"]):
    """
    Thin session manager with explicit helpers used by the scheduler, runner service,
    broker, and API layer. Returns ORM rows bound to ONE live session so subsequent
    attribute access is safe during a tick.
    """

    def __init__(self) -> None:
        self._session: Session = SessionLocal()

    # Expose the Session as `.db` for existing callsites
    @property
    def db(self) -> Session:
        return self._session

    # Context manager plumbing
    def __enter__(self) -> "DBManager":
        return self

    def __exit__(self, exc_type, exc, tb) -> None:
        try:
            if exc_type is not None:
                # Best-effort rollback; keep callers' explicit commits intact otherwise
                self._session.rollback()
        finally:
            self._session.close()

    # ───────────────────────── Users & Accounts ─────────────────────────

    def get_user_by_username(self, username: str) -> Optional[User]:
        return (
            self._session.query(User)
            .filter(User.username == username)
            .first()
        )

    def create_user(self, username: str, email: str, password: str) -> User:
        """
        Low-ceremony user creation with password hashing (bcrypt via passlib when available).
        Also ensures a mock account and simulation state exist.
        """
        if _pwd_ctx is not None:
            try:
                pw_hash = _pwd_ctx.hash(password)
            except Exception:
                pw_hash = password  # fallback – only used in dev containers
        else:
            pw_hash = password

        u = User(username=username, email=email, password_hash=pw_hash, created_at=_now_utc())
        self._session.add(u)
        self._session.commit()

        # Ensure account + simulation state
        self.ensure_account(user_id=u.id, name="mock")
        self.ensure_simulation_state(user_id=u.id)

        log.info("Created user '%s' (id=%s) with mock account and simulation state.", username, u.id)
        return u

    def get_or_create_user(self, username: str, email: str, password: str) -> User:
        """
        API gateway expects this. Idempotent: returns existing user or creates one.
        Also ensures account + simulation state.
        """
        u = self.get_user_by_username(username)
        if u:
            # Ensure bootstrap invariants even if user pre-existed
            self.ensure_account(user_id=u.id, name="mock")
            self.ensure_simulation_state(user_id=u.id)
            return u
        return self.create_user(username, email, password)

    def ensure_simulation_state(self, user_id: int) -> SimulationState:
        st = (
            self._session.query(SimulationState)
            .filter(SimulationState.user_id == user_id)
            .first()
        )
        if st:
            return st
        st = SimulationState(user_id=user_id, is_running="false", last_ts=None)
        self._session.add(st)
        self._session.commit()
        return st

    def ensure_account(self, user_id: int, name: str = "mock", cash: Optional[float] = None) -> Account:
        """
        Ensure a mock account exists.

        If the account must be created, the starting cash is:
        - the explicit `cash` argument when provided, else
        - the env `MOCK_STARTING_CASH` (default 10_000_000).

        If the account already exists and BOTH cash & equity are zero,
        and a `cash` value is provided, we backfill the balances to `cash`
        (to keep bootstrap idempotent without clobbering real balances).
        """
        acct = (
            self._session.query(Account)
            .filter(Account.user_id == user_id, Account.name == name)
            .first()
        )
        if acct:
            if cash is not None:
                try:
                    if float(acct.cash or 0.0) == 0.0 and float(acct.equity or 0.0) == 0.0:
                        acct.cash = float(cash)
                        acct.equity = float(cash)
                        self._session.commit()
                except Exception:
                    # Non-fatal: leave existing balances as-is
                    self._session.rollback()
            return acct

        initial_cash = float(cash if cash is not None else os.getenv("MOCK_STARTING_CASH", "10000000"))
        acct = Account(
            user_id=user_id,
            name=name,
            cash=initial_cash,
            equity=initial_cash,
            created_at=_now_utc(),
        )
        self._session.add(acct)
        self._session.commit()
        return acct

    # ───────────────────────── Runners ─────────────────────────

    def get_runners_by_user(
        self,
        user_id: int,
        activation: Optional[str] = None,
    ) -> List[Runner]:
        """
        Returns ATTACHED ORM Runner rows (never Core RowMappings / dicts).
        Keeping ordering stable helps debugging & determinism in sims.
        """
        q = self._session.query(Runner).filter(Runner.user_id == user_id)
        if activation:
            q = q.filter(Runner.activation == activation)
        return q.order_by(Runner.created_at.asc(), Runner.id.asc()).all()

    # ───────────────────────── Positions ─────────────────────────

    def get_open_position(self, runner_id: int) -> Optional[OpenPosition]:
        return (
            self._session.query(OpenPosition)
            .filter(OpenPosition.runner_id == runner_id)
            .first()
        )

    # ───────────────────────── Executions & results ─────────────────────────

    def record_runner_execution(
        self,
        *,
        runner_id: int,
        user_id: int,
        symbol: str,
        strategy: str,
        status: str,
        reason: Optional[str] = None,
        details: Optional[str] = None,
        execution_time: Optional[datetime] = None,
        cycle_seq: Optional[int] = None,
    ) -> RunnerExecution:
        """
        Persist a per-tick execution record. Commits immediately to ensure logs
        are visible even if later work fails within the same tick.
        Also mirrors a compact line to the 'runner-executions' logger.
        """
        if execution_time is None:
            execution_time = _now_utc()
        if cycle_seq is None:
            cycle_seq = int(execution_time.timestamp())

        rec = RunnerExecution(
            runner_id=runner_id,
            user_id=user_id,
            symbol=symbol.upper(),
            strategy=strategy,
            status=status,
            reason=reason,
            details=details,
            cycle_seq=cycle_seq,
            execution_time=execution_time,
            created_at=_now_utc(),
        )
        self._session.add(rec)
        self._session.commit()

        # Mirror to dedicated log for easy human review
        try:
            _exec_log.info(
                "cycle=%s time=%s runner=%s user=%s symbol=%s strategy=%s status=%s reason=%s",
                cycle_seq,
                execution_time.isoformat(),
                runner_id,
                user_id,
                rec.symbol,
                strategy,
                status,
                (reason or "")
            )
        except Exception:
            pass

        return rec

    # ───────────────────────── Misc helpers (used by other parts) ─────────────────────────

    def count_minute_bars(self, *, symbol: str, interval_min: int, ts_lte: datetime) -> int:
        from database.models import HistoricalMinuteBar
        stmt = (
            select(func.count())
            .select_from(HistoricalMinuteBar)
            .where(HistoricalMinuteBar.symbol == symbol.upper())
            .where(HistoricalMinuteBar.interval_min == int(interval_min))
            .where(HistoricalMinuteBar.ts <= ts_lte)
        )
        return int(self._session.execute(stmt).scalar() or 0)


    def bulk_record_runner_executions(self, records: List[Dict[str, Any]]) -> None:
        """
        Fast path: insert many RunnerExecution rows in one commit.

        Expected keys per record:
          runner_id, user_id, symbol, strategy, status, reason, details,
          execution_time (datetime), cycle_seq (int)

        Notes:
          • `symbol` is uppercased here.
          • `created_at` is set to now.
          • Mirrors compact lines to the 'runner-executions' logger, but
            avoids per-row flush/commit overhead.
        """
        if not records:
            return

        objs: List[RunnerExecution] = []
        now = _now_utc()
        for rec in records:
            try:
                objs.append(
                    RunnerExecution(
                        runner_id=int(rec.get("runner_id", 0)),
                        user_id=int(rec.get("user_id", 0)),
                        symbol=str(rec.get("symbol", "UNKNOWN")).upper(),
                        strategy=str(rec.get("strategy", "")),
                        status=str(rec.get("status", "")),
                        reason=rec.get("reason"),
                        details=rec.get("details"),
                        cycle_seq=int(rec.get("cycle_seq", int(now.timestamp()))),
                        execution_time=rec.get("execution_time") or now,
                        created_at=now,
                    )
                )
            except Exception:
                # skip malformed rows safely
                continue

        if not objs:
            return

        self._session.add_all(objs)
        self._session.commit()

        # Lightweight mirror log (aggregated to reduce I/O)
        try:
            for o in objs:
                _exec_log.info(
                    "cycle=%s time=%s runner=%s user=%s symbol=%s strategy=%s status=%s reason=%s",
                    o.cycle_seq,
                    o.execution_time.isoformat(),
                    o.runner_id,
                    o.user_id,
                    o.symbol,
                    o.strategy,
                    o.status,
                    (o.reason or "")
                )
        except Exception:
            pass


===== backend/database/__init__.py =====

DESCRIPTION: Source file



===== backend/strategies/chatgpt_5_strategy.py =====

DESCRIPTION: Source file

from __future__ import annotations

import os
import logging
from typing import Any, Dict

from backend.ib_manager.market_data_manager import MarketDataManager
from backend.strategies.explain import format_checklist
from backend.strategies.runner_decision_info import RunnerDecisionInfo

log = logging.getLogger("chatgpt-5-strategy")


class ChatGPT5Strategy:
    """
    Long-only trend breakout with:
      • Donchian breakout trigger + long MA trend filter
      • RSI momentum window
      • ATR-based trailing stop attached on BUY (analytics-friendly)
    """

    name = "ChatGPT5Strategy"

    breakout_lookback = 20
    long_ma_period = 20
    atr_period = 14
    rsi_period = 14
    rsi_min = 50.0
    rsi_max = 80.0

    # Expressed in PERCENT (e.g., 0.10 -> 0.10%)
    buy_buffer_pct = 0.10

    trail_min_pct = 0.75
    trail_max_pct = 8.0

    limit_wiggle_rth = 0.0005
    limit_wiggle_xrth = float(os.getenv("XRTH_LIMIT_WIGGLE", "0.02"))

    def __init__(self, market_data: MarketDataManager | None = None) -> None:
        self.mkt = market_data or MarketDataManager()

    # ─────────────────────────── BUY ────────────────────────────
    def decide_buy(self, info: RunnerDecisionInfo) -> Dict[str, Any]:
        symbol = (getattr(info.runner, "stock", None) or "").upper()
        price = float(info.current_price)
        candles = info.candles or []

        min_bars = max(
            self.breakout_lookback + 1,
            self.long_ma_period + 1,
            self.atr_period + 1,
            self.rsi_period + 1,
        )
        if len(candles) < min_bars:
            res = {
                "action": "NO_ACTION",
                "reason": "insufficient_data",
                "price": round(price, 4),
                "candles_count": len(candles),
                "required_bars": min_bars,
                "explanation": f"Need ≥{min_bars} bars",
                "checks": [
                    {"label": "Minimum bars", "ok": False, "actual": len(candles), "wanted": min_bars, "direction": ">="}
                ],
            }
            log.info(
                "%s NO_ACTION - insufficient_data @ %s (required=%d have=%d)",
                symbol, res["price"], min_bars, len(candles)
            )
            return res

        upper, lower = self.mkt.donchian_channel(candles, self.breakout_lookback)
        long_ma = self.mkt.calculate_sma(candles, self.long_ma_period)
        atr_val = self.mkt.calculate_atr(candles, period=self.atr_period)
        rsi_val = self.mkt.calculate_rsi(candles, period=self.rsi_period)

        if upper is None or long_ma != long_ma or atr_val != atr_val or rsi_val != rsi_val:
            res = {
                "action": "NO_ACTION",
                "reason": "indicator_unavailable",
                "price": round(price, 4),
                "explanation": "Missing/NaN indicators",
                "checks": [
                    {"label": "Indicators available", "ok": False, "actual": "NaN/missing", "wanted": "valid"}
                ],
            }
            log.info("%s NO_ACTION - indicator_unavailable @ %s", symbol, res["price"])
            return res

        # percent-based buffer (e.g., 0.10 -> 0.10%)
        breakout_level = upper * (1.0 + (self.buy_buffer_pct / 100.0))
        trend_ok = price > long_ma
        breakout_ok = price >= breakout_level
        momentum_ok = self.rsi_min <= rsi_val <= self.rsi_max

        checklist = [
            {"label": "Trend (price > MA long)", "ok": trend_ok, "actual": price, "wanted": long_ma},
            {"label": "Breakout (price ≥ Donchian+buf)", "ok": breakout_ok, "actual": price, "wanted": breakout_level},
            {"label": "Momentum (RSI range)", "ok": momentum_ok, "actual": rsi_val, "wanted": (self.rsi_min, self.rsi_max), "direction": "range"},
        ]

        if not all(i["ok"] for i in checklist):
            res = {
                "action": "NO_ACTION",
                "reason": "conditions_not_met",
                "price": round(price, 4),
                "explanation": format_checklist(checklist),
                "checks": checklist,
            }
            log.info("%s NO_ACTION - conditions_not_met @ %s", symbol, res["price"])
            return res

        trail_pct = min(max((atr_val / price) * 100.0, self.trail_min_pct), self.trail_max_pct)
        session = getattr(self.mkt, "_last_session", (None, "regular-hours"))[1]
        wiggle = self.limit_wiggle_xrth if session == "extended-hours" else self.limit_wiggle_rth
        limit_price = price * (1 + wiggle)

        res = {
            "action": "BUY",
            "order_type": "LMT",
            "price": round(price, 4),
            "limit_price": round(limit_price, 4),
            "trail_stop_order": {
                "action": "SELL",
                "order_type": "TRAIL_LIMIT",
                "trailing_percent": round(trail_pct, 2),
            },
            "explanation": format_checklist(checklist),
            "checks": checklist,
        }
        log.debug(
            "BUY %s price=%s limit=%s trail_pct=%s",
            symbol,
            res["price"],
            res["limit_price"],
            res["trail_stop_order"]["trailing_percent"],
        )
        return res

    # ─────────────────────────── SELL ───────────────────────────
    def decide_sell(self, info: RunnerDecisionInfo) -> Dict[str, Any]:
        symbol = (getattr(info.runner, "stock", None) or "").upper()
        price = float(info.current_price)
        candles = info.candles or []

        if len(candles) < self.atr_period + 1:
            res = {
                "action": "NO_ACTION",
                "reason": "insufficient_data",
                "price": round(price, 4),
                "explanation": f"Need ≥{self.atr_period + 1} bars for ATR",
                "checks": [
                    {"label": "Minimum bars for ATR", "ok": False, "actual": len(candles), "wanted": self.atr_period + 1, "direction": ">="}
                ],
            }
            log.info(
                "%s NO_ACTION - insufficient_data @ %s (required=%d have=%d)",
                symbol, res["price"], self.atr_period + 1, len(candles)
            )
            return res

        atr_val = self.mkt.calculate_atr(candles, period=self.atr_period)
        if atr_val != atr_val:  # NaN
            return {
                "action": "NO_ACTION",
                "reason": "indicator_unavailable",
                "price": round(price, 4),
                "checks": [{"label": "ATR valid", "ok": False, "actual": "NaN", "wanted": "valid"}],
            }

        trail_pct = min(max((atr_val / price) * 100.0, self.trail_min_pct), self.trail_max_pct)
        session = getattr(self.mkt, "_last_session", (None, "regular-hours"))[1]
        wiggle = self.limit_wiggle_xrth if session == "extended-hours" else self.limit_wiggle_rth
        limit_price = price * (1 - wiggle)

        res = {
            "action": "SELL",
            "order_type": "LMT",
            "price": round(price, 4),
            "limit_price": round(limit_price, 4),
            "trail_percent": round(trail_pct, 2),
            "explanation": f"Trailing stop at {trail_pct:.2f}%",
            "checks": [{"label": "ATR-based trail", "ok": True, "actual": trail_pct, "wanted": "within min/max"}],
        }
        log.debug("SELL %s price=%s limit=%s trail_pct=%s", symbol, res["price"], res["limit_price"], res["trail_percent"])
        return res


===== backend/strategies/explain.py =====

DESCRIPTION: Source file

from __future__ import annotations

from typing import Any, Dict, Iterable, Tuple


def _format_number(name: str, value: Any) -> str:
    try:
        v = float(value)
    except Exception:
        return str(value)

    lname = name.lower()
    if "rsi" in lname:
        return f"{v:.1f}"
    # default to 2 decimals for price-like numbers
    return f"{v:.2f}"


def _relation_symbol(actual: float, wanted: float) -> str:
    try:
        a = float(actual)
        w = float(wanted)
    except Exception:
        return "?"
    if a < w:
        return "<"
    if a > w:
        return ">"
    return "="


def format_actual_vs_wanted(pairs: Iterable[Dict[str, Any]]) -> str:
    """
    Build a compact explanation string like:
    "actual price 50.00 < wanted trigger 60.00 | actual rsi 40.0 < wanted rsi_min 50.0"

    Each pair item supports keys:
      - actual_label: str
      - actual: number
      - wanted_label: str
      - wanted: number | (min, max) when direction == "range"
      - direction: one of ">=", "<=", "range" (used to label the target side)
    """
    parts: list[str] = []
    for p in pairs:
        direction = str(p.get("direction", ">=")).lower()
        actual_label = str(p.get("actual_label", "value"))
        wanted_label = str(p.get("wanted_label", "target"))
        actual_val = p.get("actual")
        wanted_val = p.get("wanted")

        if direction == "range":
            try:
                low, high = wanted_val  # type: ignore[misc]
            except Exception:
                # fallback to simple representation
                parts.append(
                    f"actual {actual_label} {_format_number(actual_label, actual_val)} vs wanted {wanted_label} {_format_number(wanted_label, wanted_val)}"
                )
                continue

            a = float(actual_val)
            if a < float(low):
                parts.append(
                    f"actual {actual_label}: {_format_number(actual_label, a)} < wanted {wanted_label} min: {_format_number(wanted_label, low)}"
                )
            elif a > float(high):
                parts.append(
                    f"actual {actual_label}: {_format_number(actual_label, a)} > wanted {wanted_label} max: {_format_number(wanted_label, high)}"
                )
            else:
                parts.append(
                    f"actual {actual_label}: {_format_number(actual_label, a)} within wanted {wanted_label}: [{_format_number(wanted_label, low)}..{_format_number(wanted_label, high)}]"
                )
            continue

        rel = _relation_symbol(actual_val, wanted_val)
        parts.append(
            f"actual {actual_label}: {_format_number(actual_label, actual_val)} {rel} wanted {wanted_label}: {_format_number(wanted_label, wanted_val)}"
        )

    return " | ".join(parts)


def format_checklist(items: Iterable[Dict[str, Any]]) -> str:
    """
    Build a vertical checklist where each line is either ✅ (ok) or ❌ (failed).

    Each item supports keys:
      - label: str (mandatory)
      - ok: bool (mandatory)
      - actual: number | None
      - wanted: number | (min, max) when direction == "range"
      - direction: one of ">=", "<=", "range" (default ">=")
      - wanted_label: optional str (defaults to label)

    Rules:
      - ok=True → "✅ label: <actual>"
      - ok=False → "❌ label: <actual> <rel> wanted <wanted>" (or range message)
    """
    lines: list[str] = []
    for it in items:
        label = str(it.get("label", "value"))
        ok = bool(it.get("ok", False))
        direction = str(it.get("direction", ">=")).lower()
        actual = it.get("actual")
        wanted = it.get("wanted")
        wanted_label = str(it.get("wanted_label", label))

        if ok:
            if actual is None:
                lines.append(f"✅ {label}")
            else:
                lines.append(f"✅ {label}: {_format_number(label, actual)}")
            continue

        # failed case
        if direction == "range":
            try:
                low, high = wanted  # type: ignore[misc]
                a = float(actual)
                if a < float(low):
                    lines.append(
                        f"❌ {label}: {_format_number(label, a)} < wanted {wanted_label} min: {_format_number(wanted_label, low)}"
                    )
                elif a > float(high):
                    lines.append(
                        f"❌ {label}: {_format_number(label, a)} > wanted {wanted_label} max: {_format_number(wanted_label, high)}"
                    )
                else:
                    lines.append(
                        f"❌ {label}: {_format_number(label, a)} outside wanted {wanted_label}: [{_format_number(wanted_label, low)}..{_format_number(wanted_label, high)}]"
                    )
            except Exception:
                lines.append(
                    f"❌ {label}: {_format_number(label, actual)} vs wanted {wanted_label}: {_format_number(wanted_label, wanted)}"
                )
            continue

        rel = _relation_symbol(actual, wanted)
        lines.append(
            f"❌ {label}: {_format_number(label, actual)} {rel} wanted {wanted_label}: {_format_number(wanted_label, wanted)}"
        )

    return "\n".join(lines)




===== backend/strategies/factory.py =====

DESCRIPTION: Source file

from __future__ import annotations

"""
Dynamic strategy discovery & resolution.

Drop a new strategy file under backend/strategies/ that defines a class with:
  - decide_buy(self, info)
  - decide_sell(self, info)
  - optional: name (for nicer aliasing)
  - optional: aliases (list[str]) extra keys mapping to this strategy

This module will:
  • Discover it automatically
  • Expose a stable canonical key based on the module file name (e.g., "grok_4_strategy")
  • Allow common aliases ("grok4", "grok", snake-cased class name, etc.)
"""

import importlib
import inspect
import os
import pkgutil
import re
import logging
from types import ModuleType
from typing import Type, Any, Optional

# Logger for this module
log = logging.getLogger("strategy-factory")

# Internal registries
_CLASSES: dict[str, Type[Any]] = {}   # canonical_key -> class
_ALIASES: dict[str, str] = {}         # alias -> canonical_key
_DISCOVERED: bool = False

# Files to ignore when importing strategy modules
_SKIP_MODULES = {
    "factory",
    "contracts",
    "runner_decision_info",
    "explain",
    "__init__",
}


def _snake_case(name: str) -> str:
    s1 = re.sub("(.)([A-Z][a-z]+)", r"\1_\2", name)
    s2 = re.sub("([a-z0-9])([A-Z])", r"\1_\2", s1)
    out = re.sub("__+", "_", s2).lower()
    return out


def _ensure_strategy_suffix(key: str) -> str:
    k = key.lower()
    if not k.endswith("_strategy"):
        k = f"{k}_strategy"
    return k


def _add_alias(alias: str, canonical: str) -> None:
    alias = alias.strip().lower()
    if not alias:
        return
    _ALIASES[alias] = canonical


def _discover() -> None:
    """Populate _CLASSES and _ALIASES once per process."""
    global _DISCOVERED
    if _DISCOVERED:
        return

    pkg_dir = os.path.dirname(__file__)
    pkg_name = __package__  # "backend.strategies"

    for modinfo in pkgutil.iter_modules([pkg_dir]):
        mod_name = modinfo.name
        if mod_name in _SKIP_MODULES or mod_name.startswith("_"):
            continue

        try:
            mod: ModuleType = importlib.import_module(f"{pkg_name}.{mod_name}")
        except Exception as e:
            # Skip broken modules but emit a debug so issues aren't invisible
            log.debug("Skipping strategy module %s due to import error: %s", mod_name, e)
            continue

        # Canonical key is the *module filename* (e.g., grok_4_strategy)
        canonical_key = mod_name.lower()

        # Find classes in module that look like strategies
        for _, cls in inspect.getmembers(mod, inspect.isclass):
            if cls.__module__ != mod.__name__:
                continue
            if not hasattr(cls, "decide_buy") or not hasattr(cls, "decide_sell"):
                continue  # not a strategy

            # Register the class exactly once per canonical module key
            if canonical_key not in _CLASSES:
                _CLASSES[canonical_key] = cls

                # Build a rich set of aliases
                class_name_snake = _snake_case(getattr(cls, "name", cls.__name__))
                class_alias = _ensure_strategy_suffix(class_name_snake)

                # Primary aliases
                _add_alias(canonical_key, canonical_key)
                _add_alias(class_alias, canonical_key)

                # Loose variants for convenience
                _add_alias(class_alias.replace("_strategy", ""), canonical_key)     # e.g., "grok_4"
                _add_alias(class_alias.replace("_", ""), canonical_key)            # e.g., "chatgpt5strategy"
                _add_alias(class_alias.replace("_strategy", "").replace("_", ""), canonical_key)  # e.g., "grok4"

                # Any explicit aliases on the class
                for a in getattr(cls, "aliases", []):
                    _add_alias(a, canonical_key)

            # We only need one class per module as the canonical strategy class
            break

    _DISCOVERED = True


def list_available_strategy_keys() -> list[str]:
    """Canonical strategy keys (module filenames), e.g. ["chatgpt_5_strategy", "grok_4_strategy"]."""
    _discover()
    return sorted(_CLASSES.keys())


def resolve_strategy_key(key: str | None) -> Optional[str]:
    """Return the canonical key for any alias, or None if unknown."""
    if not key:
        return None
    _discover()
    k = key.strip().lower()
    return _ALIASES.get(k)


def select_strategy(runner) -> Any:
    """
    Resolve runner.strategy → strategy instance.
    Raises ValueError if unknown.
    """
    _discover()
    canonical = resolve_strategy_key(getattr(runner, "strategy", None))
    if not canonical or canonical not in _CLASSES:
        raise ValueError(f"Unknown or unsupported strategy '{getattr(runner, 'strategy', None)}'")
    return _ClassesFactory.create(canonical)


class _ClassesFactory:
    """Tiny helper to keep instantiation logic in one place (future DI etc.)."""

    @staticmethod
    def create(canonical_key: str) -> Any:
        cls = _CLASSES[canonical_key]
        return cls()  # strategies accept no required ctor args today


===== backend/strategies/contracts.py =====

DESCRIPTION: Source file

from __future__ import annotations
from typing import Protocol, Literal, TypedDict, Any, Dict

# ───────── Strategy decision shapes ─────────
ActionT = Literal["BUY", "SELL", "NO_ACTION"]

class TrailStopSpec(TypedDict, total=False):
    trailing_percent: float | None
    trailing_amount: float | None
    limit_offset: float | None

class StaticStopSpec(TypedDict, total=False):
    action: str
    order_type: str
    stop_price: float | None
    limit_price: float | None

class StrategyDecision(TypedDict, total=False):
    action: ActionT
    reason: str
    order_type: str
    quantity: int
    limit_price: float | None
    stop_price: float | None
    trail_stop_order: TrailStopSpec  # Trailing stop for BUY
    static_stop_order: StaticStopSpec  # Static stop for BUY (alternative to trail)

class Strategy(Protocol):
    name: str
    def decide_buy(self, info: "RunnerDecisionInfo") -> StrategyDecision: ...
    def decide_sell(self, info: "RunnerDecisionInfo") -> StrategyDecision: ...

# ───────── Validation ─────────
class StrategyDecisionError(ValueError):
    pass

def validate_decision(decision: Dict[str, Any] | None, *, is_exit: bool) -> StrategyDecision | None:
    if not decision or decision.get("action") in (None, "", "NO_ACTION"):
        # Preserve all fields from the original decision, not just action and reason
        if decision:
            return dict(decision)  # Return the full decision with all details
        return {"action": "NO_ACTION", "reason": "no_signal"}

    out: Dict[str, Any] = dict(decision)
    action = str(out.get("action", "")).upper()
    out["action"] = action

    if action not in {"BUY", "SELL", "NO_ACTION"}:
        raise StrategyDecisionError(f"Invalid action '{action}'")

    if action == "NO_ACTION":
        # Preserve all fields from the original decision, not just action and reason
        return out  # Return the full decision with all details

    # quantity (when provided)
    if "quantity" in out:
        try:
            q = int(out["quantity"])
            if q <= 0:
                raise StrategyDecisionError("quantity must be > 0 when provided")
            out["quantity"] = q
        except Exception as e:
            raise StrategyDecisionError("quantity must be an integer > 0") from e

    # LMT requires limit_price
    ot = out.get("order_type")
    if ot is not None and str(ot).upper() == "LMT":
        lp = out.get("limit_price")
        try:
            if lp is None or float(lp) <= 0:
                raise StrategyDecisionError("LMT orders require a positive limit_price")
        except Exception as e:
            raise StrategyDecisionError("limit_price must be a positive number for LMT") from e

    # BUY must include either trailing stop or static stop in live mode
    if action == "BUY":
        ts = out.get("trail_stop_order")
        ss = out.get("static_stop_order")

        # In analytics-only mode we allow BUY without stop specs to keep simulations simple.
        # Be tolerant: if RUNNING_ENV is missing, default to analytics to avoid accidental strict blocking
        import os
        running_env = os.getenv("RUNNING_ENV", "analytics").lower()
        if running_env == "analytics":
            # nothing to enforce here for analytics
            pass
        else:
            if not isinstance(ts, dict) and not isinstance(ss, dict):
                raise StrategyDecisionError("BUY decision must include either 'trail_stop_order' or 'static_stop_order' dict")
        
        # Validate trailing stop if provided
        if isinstance(ts, dict):
            tp = ts.get("trailing_percent")
            ta = ts.get("trailing_amount")
            if (tp is None or float(tp) <= 0) and (ta is None or float(ta) <= 0):
                raise StrategyDecisionError(
                    "trail_stop_order must include positive trailing_percent or trailing_amount"
                )
        
        # Validate static stop if provided
        if isinstance(ss, dict):
            stop_price = ss.get("stop_price")
            order_type = ss.get("order_type", "").upper()
            if stop_price is None or float(stop_price) <= 0:
                raise StrategyDecisionError("static_stop_order must include positive stop_price")
            if order_type not in {"STOP", "STOP_LIMIT"}:
                raise StrategyDecisionError("static_stop_order order_type must be 'STOP' or 'STOP_LIMIT'")
            if order_type == "STOP_LIMIT":
                limit_price = ss.get("limit_price")
                if limit_price is None or float(limit_price) <= 0:
                    raise StrategyDecisionError("STOP_LIMIT static_stop_order must include positive limit_price")

    return out  # type: ignore[return-value]


===== backend/strategies/runner_decision_info.py =====

DESCRIPTION: Source file

from __future__ import annotations
from dataclasses import dataclass
from typing import List, Dict, Optional
from database.models import Runner, OpenPosition

@dataclass(slots=True)
class RunnerDecisionInfo:
    """
    Tiny, immutable context object passed to strategies.
    """
    runner: Runner
    position: Optional[OpenPosition]
    current_price: float
    candles: List[Dict]  # last ≤250 bars
    distance_from_time_limit: Optional[float]


===== backend/strategies/grok_4_strategy.py =====

DESCRIPTION: Source file

from __future__ import annotations

import os
from math import floor
from typing import Any, Dict

from backend.ib_manager.market_data_manager import MarketDataManager
from backend.strategies.explain import format_checklist
from backend.strategies.runner_decision_info import RunnerDecisionInfo
import logging

log = logging.getLogger("grok-4-strategy")

class Grok4Strategy:
    """
    Advanced long-only strategy combining Fibonacci extensions, trend breakouts, 
    RSI momentum, and ATR-based risk management for high gains with low risk.
    Designed for frequent triggers on good stocks.
    
    Key features:
    - Multi-timeframe trend confirmation
    - Dynamic Fibonacci entries with reduced offsets for more triggers
    - Momentum filters with wider RSI range
    - Adaptive ATR trailing stops
    - Volume confirmation for breakouts
    
    All parameters hardcoded for minimal user input.
    """
    
    name = "Grok4Strategy"
    
    # Internal config - shorter periods for more frequent signals
    ma_short_period = 20      # Short MA for momentum
    ma_long_period = 20       # Long MA for trend (easier data requirement)
    rsi_period = 14
    rsi_low = 40.0            # Lower threshold for more entries
    rsi_high = 75.0           # Avoid extreme overbought
    atr_period = 14
    fib_offset_ratio = 0.10   # Reduced for more triggers
    volume_ma_period = 20     # Volume confirmation
    trail_min_pct = 0.5
    trail_max_pct = 6.0
    limit_wiggle_rth = 0.0005
    limit_wiggle_xrth = float(os.getenv("XRTH_LIMIT_WIGGLE", "0.02"))
    
    def __init__(self, market_data: MarketDataManager | None = None) -> None:
        self.mkt = market_data or MarketDataManager()
    
    def decide_buy(self, info: RunnerDecisionInfo) -> Dict[str, Any]:
        symbol = (getattr(info.runner, "stock", None) or "").upper()
        price = float(info.current_price)
        candles = info.candles or []
        
        min_bars = max(
            self.ma_long_period + 1,
            self.rsi_period + 1,
            self.atr_period + 1,
            self.volume_ma_period + 1,
        )
        if len(candles) < min_bars:
            res = {
                "action": "NO_ACTION",
                "reason": "insufficient_data",
                "explanation": f"Need at least {min_bars} bars",
                "checks": [
                    {"label": "Minimum bars", "ok": False, "actual": len(candles), "wanted": min_bars, "direction": ">="}
                ],
            }
            log.debug("Grok4Strategy.decide_buy insufficient data symbol=%s required=%d have=%d", symbol, min_bars, len(candles))
            return res
        
        # Calculate indicators
        ma_short = self.mkt.calculate_ema(candles, self.ma_short_period)
        ma_long = self.mkt.calculate_sma(candles, self.ma_long_period)
        rsi = self.mkt.calculate_rsi(candles, self.rsi_period)
        atr = self.mkt.calculate_atr(candles, self.atr_period)
        volume_ma = self.mkt.average_volume(candles, self.volume_ma_period)
        
        # Simple fib calculation - adapt from fibonacci strategy
        high = max(c['high'] for c in candles[-50:])
        low = min(c['low'] for c in candles[-50:])
        fib_618 = high - (high - low) * 0.618
        entry_level = fib_618 * (1 + self.fib_offset_ratio)
        
        # Conditions - looser for more triggers
        trend_ok = price > ma_long and ma_short > ma_long
        momentum_ok = self.rsi_low < rsi < self.rsi_high
        volume_ok = candles[-1]['volume'] > volume_ma * 1.2  # 20% above MA
        fib_ok = price > entry_level
        
        checklist = [
            {"label": "Trend (price > MA long)", "ok": trend_ok, "actual": price, "wanted": ma_long},
            {"label": "Momentum (RSI in range)", "ok": momentum_ok, "actual": rsi, "wanted": (self.rsi_low, self.rsi_high), "direction": "range"},
            {"label": "Volume breakout", "ok": volume_ok, "actual": candles[-1]['volume'], "wanted": volume_ma * 1.2},
            {"label": "Fib entry", "ok": fib_ok, "actual": price, "wanted": entry_level},
        ]
        
        if not all(item['ok'] for item in checklist):
            res = {
                "action": "NO_ACTION",
                "reason": "conditions_not_met",
                "explanation": format_checklist(checklist),
                "checks": checklist,
            }
            log.debug("Grok4Strategy.decide_buy conditions not met symbol=%s checklist=%s", symbol, checklist)
            return res
        
        # Calculate order details
        trail_pct = min(max((atr / price) * 100, self.trail_min_pct), self.trail_max_pct)
        session  = self.mkt._last_session[1] if getattr(self.mkt, "_last_session", None) else None
        wiggle = self.limit_wiggle_xrth if session == "extended-hours" else self.limit_wiggle_rth
        limit_price = price * (1 + wiggle)
        
        res = {
            "action": "BUY",
            "order_type": "LMT",
            "price": round(price, 4),
            "limit_price": round(limit_price, 4),
            "trail_stop_order": {
                "action": "SELL",
                "order_type": "TRAIL_LIMIT",
                "trailing_percent": round(trail_pct, 2),
            },
            "explanation": format_checklist(checklist),
            "checks": checklist,
        }
        log.debug("Grok4Strategy.decide_buy BUY symbol=%s price=%s limit=%s trail_pct=%s", symbol, res["price"], res["limit_price"], res["trail_stop_order"]["trailing_percent"])
        return res
    
    def decide_sell(self, info: RunnerDecisionInfo) -> Dict[str, Any]:
        symbol = (getattr(info.runner, "stock", None) or "").upper()
        price = float(info.current_price)
        candles = info.candles or []
        
        if len(candles) < self.atr_period + 1:
            return {
                "action": "NO_ACTION",
                "reason": "insufficient_data",
                "explanation": f"Need at least {self.atr_period + 1} bars for ATR",
                "checks": [
                    {"label": "Minimum bars for ATR", "ok": False, "actual": len(candles), "wanted": self.atr_period + 1, "direction": ">="}
                ],
            }
        
        atr = self.mkt.calculate_atr(candles, self.atr_period)
        if atr is None:
            return {
                "action": "NO_ACTION",
                "reason": "indicator_unavailable",
                "explanation": "ATR indicator unavailable",
                "checks": [{"label": "ATR valid", "ok": False, "actual": "None", "wanted": "valid"}],
            }
        
        trail_pct = min(max((atr / price) * 100, self.trail_min_pct), self.trail_max_pct)
        session  = self.mkt._last_session[1] if getattr(self.mkt, "_last_session", None) else None
        wiggle = self.limit_wiggle_xrth if session == "extended-hours" else self.limit_wiggle_rth
        limit_price = price * (1 - wiggle)
        
        return {
            "action": "SELL",
            "order_type": "LMT",
            "price": round(price, 4),
            "limit_price": round(limit_price, 4),
            "trail_percent": round(trail_pct, 2),
            "explanation": f"SELL SIGNAL - Trailing stop at {trail_pct:.2f}% below current price",
            "checks": [{"label": "ATR-based trail", "ok": True, "actual": trail_pct, "wanted": "within min/max"}],
        }

    def decide_refresh(self, info: RunnerDecisionInfo) -> Dict[str, Any] | None:
        return {"action": "NO_ACTION", "reason": "no_refresh_logic"}


===== backend/analytics_importer.py =====

DESCRIPTION: Source file

from __future__ import annotations
import os
import sqlite3
import logging
from datetime import datetime, timezone
from typing import Iterable

from sqlalchemy.dialects.postgresql import insert
from sqlalchemy import select, func

from database.db_core import engine
from database.models import HistoricalDailyBar, HistoricalMinuteBar

logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)-8s %(name)s: %(message)s',
    handlers=[logging.FileHandler('/app/logs/analytics_importer.log'), logging.StreamHandler()]
)
logger = logging.getLogger('analytics_importer')

# Prefer container path; can still be overridden via env
SQLITE_PATH = os.getenv(
    "ANALYTICS_SQLITE_PATH",
    "/app/tools/finnhub_downloader/data/daily_bars.sqlite",
)


def _yield_daily_rows(cur) -> Iterable[dict]:
    for sym, date_epoch, o, h, l, c, v in cur:
        yield {
            "symbol": str(sym).upper(),
            "date": datetime.fromtimestamp(int(date_epoch), tz=timezone.utc),
            "open": float(o),
            "high": float(h),
            "low": float(l),
            "close": float(c),
            "volume": int(v),
        }


def _yield_minute_rows(cur) -> Iterable[dict]:
    for sym, ts_epoch, interval_min, o, h, l, c, v in cur:
        yield {
            "symbol": str(sym).upper(),
            "ts": datetime.fromtimestamp(int(ts_epoch), tz=timezone.utc),
            "interval_min": int(interval_min),
            "open": float(o),
            "high": float(h),
            "low": float(l),
            "close": float(c),
            "volume": int(v),
        }


def _upsert_daily(pg_conn, rows: list[dict]) -> None:
    if not rows:
        return
    ins = insert(HistoricalDailyBar).values(rows)
    update_cols = {c.name: getattr(ins.excluded, c.name) for c in HistoricalDailyBar.__table__.columns if c.name != "id"}
    pg_conn.execute(ins.on_conflict_do_update(index_elements=["symbol", "date"], set_=update_cols))


def _upsert_minute(pg_conn, rows: list[dict]) -> None:
    if not rows:
        return
    ins = insert(HistoricalMinuteBar).values(rows)
    update_cols = {c.name: getattr(ins.excluded, c.name) for c in HistoricalMinuteBar.__table__.columns if c.name != "id"}
    pg_conn.execute(ins.on_conflict_do_update(index_elements=["symbol", "ts", "interval_min"], set_=update_cols))


def import_sqlite(sqlite_path: str = SQLITE_PATH, batch_size: int = 5000) -> None:
    """
    Idempotent importer:
      • First checks Postgres for existing data; if present, skips without requiring the SQLite file.
      • If DB is empty, requires a readable SQLite file and imports daily + 5m bars.
      • Creates a simple 'import completed' marker to avoid repeat work in the same container.
    """
    logger.info("=== Starting Analytics Data Import ===")
    logger.info("SQLite source: %s", sqlite_path)

    import_marker = "/app/data/.import_completed"

    # 0) Quick DB check — skip early if data exists
    try:
        with engine.connect() as pg_check:
            daily_ct = int(pg_check.execute(select(func.count()).select_from(HistoricalDailyBar)).scalar() or 0)
            minute_ct = int(pg_check.execute(select(func.count()).select_from(HistoricalMinuteBar)).scalar() or 0)
            if (daily_ct + minute_ct) > 0:
                logger.info("Existing historical data detected in Postgres (daily=%d, minute=%d) — skipping import.", daily_ct, minute_ct)
                # Create/refresh marker for observability
                os.makedirs(os.path.dirname(import_marker), exist_ok=True)
                with open(import_marker, "w") as f:
                    f.write("Import skipped: data already present")
                return
    except Exception as e:
        logger.warning("Pre-check of existing data failed (tables may not exist yet): %s", e)

    # 1) Container-scoped marker (best-effort)
    if os.path.exists(import_marker):
        logger.info("Import marker present at %s — assuming already imported for this container.", import_marker)
        return

    # 2) Only now require the SQLite file (DB was empty)
    if not os.path.exists(sqlite_path):
        raise FileNotFoundError(f"SQLite file not found: {sqlite_path}")

    # 3) Perform import
    logger.info("Connecting to SQLite database...")
    conn = sqlite3.connect(sqlite_path)
    try:
        with engine.begin() as pg:
            # Daily bars
            logger.info("=== Importing Daily Bars ===")
            cur = conn.cursor()
            cur.execute("SELECT symbol, date, open, high, low, close, volume FROM daily_bars ORDER BY symbol, date")
            buf: list[dict] = []
            count = 0
            for row in _yield_daily_rows(cur):
                buf.append(row)
                if len(buf) >= batch_size:
                    _upsert_daily(pg, buf)
                    count += len(buf)
                    buf.clear()
            if buf:
                _upsert_daily(pg, buf)
                count += len(buf)
            logger.info("Daily bars imported: %d", count)

            # Minute bars (5m)
            logger.info("=== Importing Minute Bars (5m) ===")
            cur = conn.cursor()
            cur.execute("SELECT symbol, ts, interval, open, high, low, close, volume FROM minute_bars WHERE interval=5 ORDER BY symbol, ts")
            buf = []
            count = 0
            for row in _yield_minute_rows(cur):
                buf.append(row)
                if len(buf) >= batch_size:
                    _upsert_minute(pg, buf)
                    count += len(buf)
                    buf.clear()
            if buf:
                _upsert_minute(pg, buf)
                count += len(buf)
            logger.info("Minute bars imported: %d", count)

    finally:
        try:
            conn.close()
        except Exception:
            pass

    # 4) Write marker
    os.makedirs(os.path.dirname(import_marker), exist_ok=True)
    with open(import_marker, "w") as f:
        f.write("Import completed")
    logger.info("=== Historical data import completed ===")


===== backend/api_gateway/main.py =====

DESCRIPTION: Source file

from __future__ import annotations

import asyncio
import logging
import os
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from api_gateway.routes import auth_routes, analytics_routes
from logger_config import setup_logging as setup_analytics_logging

# ───────── database initialisation ─────────
from sqlalchemy import inspect, text
from database.db_core import engine, wait_for_db_ready
from database.models import Base
from backend.database.init_db import _apply_light_migrations  # reuse the tiny migration

app = FastAPI()

def _configure_logging() -> None:
    setup_analytics_logging()
    # Honor LOG_LEVEL instead of forcing INFO
    lvl = getattr(logging, os.getenv("LOG_LEVEL", "INFO").upper(), logging.INFO)
    for name in ["uvicorn", "uvicorn.error", "uvicorn.access", "fastapi"]:
        lg = logging.getLogger(name)
        lg.setLevel(lvl)
        lg.propagate = True

async def _bootstrap_everything() -> None:
    """
    1) Ensure historical data imported (idempotent)
    2) Ensure mock user & account exist
    3) Ensure runners exist for each symbol for both strategies/timeframes
    4) Optionally auto-start simulation (SIM_AUTO_START=1) WITHOUT resetting last_ts backwards
    """
    log = logging.getLogger("api-gateway")
    try:
        # Make sure the DB is actually reachable before any imports/selects
        await asyncio.get_running_loop().run_in_executor(None, wait_for_db_ready)
    except Exception:
        log.exception("Database readiness check failed")
        return

    # Import daily/minute data if needed (idempotent)
    try:
        from backend.analytics_importer import import_sqlite
        await asyncio.get_running_loop().run_in_executor(None, import_sqlite)
        log.info("Historical import ensured (idempotent).")
    except Exception:
        log.exception("Historical import failed")

    try:
        from database.db_manager import DBManager
        from sqlalchemy import select, func
        from database.models import HistoricalDailyBar, HistoricalMinuteBar, SimulationState

        with DBManager() as db:
            user = db.get_or_create_user(
                username="analytics",
                email="analytics@example.com",
                password="analytics",
            )
            start_cash = float(os.getenv("SIM_START_CASH", "10000000"))
            db.ensure_account(user_id=user.id, name="mock", cash=start_cash)

            # Bootstrap runners (idempotent create attempts)
            with engine.connect() as conn:
                syms = [r[0] for r in conn.execute(
                    select(HistoricalDailyBar.symbol).distinct().order_by(HistoricalDailyBar.symbol.asc())
                ).fetchall()]

            if syms:
                strategies = ["chatgpt_5_strategy", "grok_4_strategy"]
                timeframes = [5, 1440]
                created = 0
                for sym in syms:
                    for strat in strategies:
                        for tf in timeframes:
                            name = f"{sym}-{strat}-{('5m' if tf == 5 else '1d')}"
                            try:
                                db.create_runner(
                                    user_id=user.id,
                                    data={
                                        "name": name,
                                        "strategy": strat,
                                        "budget": start_cash * 10,
                                        "stock": sym,
                                        "time_frame": tf,
                                        "parameters": {},
                                        "exit_strategy": "hold_forever",
                                        "activation": "active",
                                    },
                                )
                                created += 1
                            except Exception:
                                try:
                                    db.db.rollback()
                                except Exception:
                                    pass
                log.info("Bootstrap runners ensured; created=%d", created)
            else:
                log.warning("No symbols found; runners will be created later when data appears.")

            # Forward-only initialization of SimulationState.last_ts
            with engine.connect() as conn:
                min_ts = conn.execute(select(func.min(HistoricalMinuteBar.ts))).scalar()
                max_ts = conn.execute(select(func.max(HistoricalMinuteBar.ts))).scalar()

            st = db.db.query(SimulationState).filter(SimulationState.user_id == user.id).first()
            if not st:
                st = SimulationState(user_id=user.id, is_running="false", last_ts=None)
                db.db.add(st)
                db.db.flush()

            if min_ts and max_ts:
                warmup_bars = int(os.getenv("SIM_WARMUP_BARS", os.getenv("WARMUP_BARS", "30")))
                step_sec = int(os.getenv("SIM_STEP_SECONDS", "300"))  # default 5m

                min_epoch = int((min_ts if min_ts.tzinfo else min_ts.replace(tzinfo=timezone.utc)).timestamp())
                max_epoch = int((max_ts if max_ts.tzinfo else max_ts.replace(tzinfo=timezone.utc)).timestamp())
                desired_start_epoch = min(min_epoch + warmup_bars * step_sec, max_epoch)

                existing_epoch = (
                    int((st.last_ts if (st.last_ts and st.last_ts.tzinfo) else (st.last_ts or datetime.fromtimestamp(0, tz=timezone.utc)).replace(tzinfo=timezone.utc)).timestamp())
                    if st.last_ts else None
                )
                new_epoch = desired_start_epoch if existing_epoch is None else max(existing_epoch, desired_start_epoch)
                if existing_epoch != new_epoch:
                    st.last_ts = datetime.fromtimestamp(new_epoch, tz=timezone.utc)
                    log.info("SimulationState initialized (forward-only) to %s", st.last_ts.isoformat())
                else:
                    if st.last_ts:
                        log.info("SimulationState kept at %s (forward-only).", st.last_ts.isoformat())
            else:
                log.warning("No minute bars present; SimulationState left unchanged.")

            db.db.commit()

    except Exception:
        log.exception("Bootstrap user/account/runners/state failed")

    # Respect auto-start without touching time
    if os.getenv("SIM_AUTO_START", "0") == "1":
        try:
            # Uses forward-only start_simulation defined above
            from api_gateway.routes.analytics_routes import start_simulation
            start_simulation()
            log.info("Simulation auto-started (SIM_AUTO_START=1).")
        except Exception:
            log.exception("Auto-start failed")


@app.on_event("startup")
async def _init_db() -> None:
    _configure_logging()
    logger = logging.getLogger("api-gateway")

    # Ensure DB is reachable before we inspect/touch it
    try:
        await asyncio.get_running_loop().run_in_executor(None, wait_for_db_ready)
    except Exception as exc:
        logger.exception("DB not ready at startup: %s", exc)
        # We still continue; background tasks will retry

    try:
        inspector = inspect(engine)
        allow_create = os.getenv("DB_ALLOW_AUTO_CREATE_TABLES", "true").lower() == "true"
        if not inspector.has_table("users"):
            if allow_create:
                Base.metadata.create_all(bind=engine)
                logger.info("Created database tables (auto-create enabled)")
            else:
                logger.warning("Tables missing and auto-create disabled.")
        _apply_light_migrations()
    except Exception as exc:
        logger.exception("DB initialization failed: %s", exc)

    asyncio.create_task(_bootstrap_everything())

    try:
        external = os.getenv("EXTERNAL_SCHEDULER", "0") == "1"
        if not external:
            import backend.analytics.sim_scheduler as sim_scheduler
            asyncio.create_task(sim_scheduler.main())
            logger.info("Internal analytics scheduler started (background).")
        else:
            logger.info("External scheduler configured; internal scheduler not started.")
    except Exception:
        logger.exception("Failed to start internal analytics scheduler")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"],
)

app.include_router(auth_routes.router,      prefix="/api")
app.include_router(analytics_routes.router, prefix="/api")


===== backend/api_gateway/__init__.py =====

DESCRIPTION: Source file



===== backend/api_gateway/routes/schemas/auth.py =====

DESCRIPTION: Source file

from typing import Annotated, Optional
from pydantic import BaseModel, EmailStr, StringConstraints

Username = Annotated[str, StringConstraints(strip_whitespace=True,
                                            min_length=3, max_length=30)]
Password = Annotated[str, StringConstraints(min_length=6, max_length=50)]

class UserCreate(BaseModel):
    username: Username
    email:    EmailStr
    password: Password
    ib_username: Optional[str] = None   # ← added
    ib_password: Optional[str] = None   # ← added

class UserLogin(BaseModel):
    username: Username
    password: Password

class Token(BaseModel):
    access_token: str
    token_type:   str = "bearer"

class UserPublic(BaseModel):
    id:       int
    username: str
    email:    EmailStr

===== backend/api_gateway/routes/schemas/runner.py =====

DESCRIPTION: Source file

# api_gateway/routes/schemas/runner.py
from __future__ import annotations

from datetime import datetime, timezone
from typing import Any, Dict, Optional

from pydantic import BaseModel, Field, PositiveFloat, PositiveInt, field_validator, ConfigDict
from dateutil import parser as dtparser

class RunnerCreate(BaseModel):
    # ─────────── fields expected from UI ───────────
    id: Optional[int] = None              # ignored by backend
    created_at: Optional[datetime] = None # ignored by backend

    name: str
    strategy: str
    budget: PositiveFloat
    stock: str
    time_frame: PositiveInt

    # ─── new free-form container for strategy params ───
    parameters: Dict[str, Any] = Field(
        default_factory=dict,
        description="Strategy-specific parameters (e.g. stop_loss, take_profit, commission_ratio)",
    )

    time_range_from: Optional[datetime] = None
    time_range_to:   Optional[datetime] = None

    exit_strategy: str
    activation: str = "active"

    # accept unknown keys but ignore them
    model_config = ConfigDict(extra="ignore")

    # ─────────── validators ───────────
    @field_validator("stock")
    @classmethod
    def _upper_ticker(cls, v: str) -> str:
        return v.upper()

    @field_validator(
        "time_range_from",
        "time_range_to",
        "created_at",
        mode="before",
    )
    @classmethod
    def _parse_dt(cls, v):
        """
        Accepts:
            • null / ""                → None
            • milliseconds since epoch → aware-UTC datetime
            • ISO string               → parsed; if no tz, assume UTC
            • datetime                 → left as-is if tz-aware, else set UTC
        """
        if v is None or (isinstance(v, str) and not v.strip()):
            return None

        # 1️⃣ epoch milliseconds from JS date-picker
        if isinstance(v, (int, float)):
            return datetime.fromtimestamp(v / 1000, tz=timezone.utc)

        # 2️⃣ already a datetime object
        if isinstance(v, datetime):
            return v if v.tzinfo else v.replace(tzinfo=timezone.utc)

        # 3️⃣ ISO string
        try:
            dt = dtparser.parse(v)
            return dt if dt.tzinfo else dt.replace(tzinfo=timezone.utc)
        except Exception as exc:
            raise ValueError(f"Invalid datetime value: {v!r}") from exc


class RunnerIds(BaseModel):
    ids: list[int] = Field(..., min_items=1, description="Runner IDs to act on")


===== backend/api_gateway/routes/schemas/__init__.py =====

DESCRIPTION: Source file



===== backend/api_gateway/routes/auth_routes.py =====

DESCRIPTION: Source file

from fastapi import APIRouter, HTTPException, status, Depends

from api_gateway.routes.schemas.auth import UserCreate, UserLogin, Token, UserPublic
from api_gateway.security.auth import create_access_token, get_current_user
from database.db_manager import DBManager
import logging

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/auth", tags=["auth"])

# ───────── signup ─────────
@router.post("/signup", response_model=UserPublic, status_code=201)
def signup(payload: UserCreate):
    with DBManager() as db:
        try:
            user = db.create_user(**payload.model_dump())
            return user      # FastAPI converts via response_model
        except ValueError as e:
            raise HTTPException(status.HTTP_400_BAD_REQUEST, str(e))
        except Exception:
            logger.exception("Signup failed")
            raise HTTPException(500, "Internal error")

# ───────── login ─────────
@router.post("/login", response_model=Token)
def login(payload: UserLogin):
    try:
        with DBManager() as db:
            if (user := db.authenticate(**payload.model_dump())) is None:
                raise HTTPException(status.HTTP_401_UNAUTHORIZED,
                                    "Incorrect username or password")
            token = create_access_token(user.username)
            return {"access_token": token}
    except HTTPException:
        # Re-raise HTTP exceptions (401) as-is
        raise
    except Exception as e:
        logger.exception("Login failed due to database error")
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR,
                           "Login service temporarily unavailable")

# ───────── handy “who-am-I” endpoint ─────────
@router.get("/me", response_model=UserPublic)
def me(current=Depends(get_current_user)):
    return current

===== backend/api_gateway/routes/analytics_routes.py =====

DESCRIPTION: Source file

# FILE: backend/api_gateway/routes/analytics_routes.py
# DESCRIPTION: Minimal analytics API for server-side simulation

from __future__ import annotations
from fastapi import APIRouter, HTTPException, Query
from typing import Optional
from datetime import datetime, timezone, timedelta
from sqlalchemy import select, func, text
import os
import logging

from database.db_core import engine
from database.db_manager import DBManager
from database.models import (
    RunnerExecution,
    AnalyticsResult,
    HistoricalDailyBar,
    HistoricalMinuteBar,
    SimulationState,
    ExecutedTrade,
    Runner
)

router = APIRouter(prefix="/analytics", tags=["analytics"])


def _now_sim() -> Optional[int]:
    try:
        with DBManager() as db:
            user = db.get_user_by_username("analytics")
            st = db.db.query(SimulationState).filter(SimulationState.user_id == user.id).first() if user else None
            if st and st.last_ts:
                return int(st.last_ts.timestamp())
    except Exception:
        pass
    try:
        v = os.getenv("SIM_TIME_EPOCH")
        return int(v) if v else None
    except Exception:
        return None


@router.get("/database/status")
def get_database_status() -> dict:
    with engine.connect() as conn:
        daily = conn.execute(select(func.count()).select_from(HistoricalDailyBar)).scalar() or 0
        minute = conn.execute(select(func.count()).select_from(HistoricalMinuteBar)).scalar() or 0
        start = conn.execute(select(func.min(HistoricalMinuteBar.ts))).scalar()
        end = conn.execute(select(func.max(HistoricalMinuteBar.ts))).scalar()
    with DBManager() as db:
        users = db.count_users()
        runners = db.count_runners()
    return {
        "data": {"daily_bars": int(daily), "minute_bars": int(minute),
                 "date_range": {"start": start.isoformat() if start else None,
                                "end": end.isoformat() if end else None}},
        "setup": {"users": users, "runners": runners},
        "ready": (daily > 0 and minute > 0 and users > 0 and runners > 0)
    }


@router.post("/simulation/start")
def start_simulation() -> dict:
    logger = logging.getLogger("api-gateway")
    try:
        # Discover 5m boundaries
        with engine.connect() as conn:
            min_ts = conn.execute(select(func.min(HistoricalMinuteBar.ts))).scalar()
            max_ts = conn.execute(select(func.max(HistoricalMinuteBar.ts))).scalar()

        if not min_ts or not max_ts:
            raise HTTPException(status_code=400, detail="No historical minute data found")

        # Warmup-aware desired start (forward-only)
        warmup_bars = int(os.getenv("SIM_WARMUP_BARS", os.getenv("WARMUP_BARS", "30")))
        step_sec = int(os.getenv("SIM_STEP_SECONDS", "300"))  # default 5m

        min_epoch = int((min_ts if min_ts.tzinfo else min_ts.replace(tzinfo=timezone.utc)).timestamp())
        max_epoch = int((max_ts if max_ts.tzinfo else max_ts.replace(tzinfo=timezone.utc)).timestamp())
        desired_start_epoch = min(min_epoch + warmup_bars * step_sec, max_epoch)

        with DBManager() as db:
            user = db.get_or_create_user("analytics", "analytics@example.com", "analytics")
            st = db.db.query(SimulationState).filter(SimulationState.user_id == user.id).first()

            if not st:
                st = SimulationState(
                    user_id=user.id,
                    is_running="true",
                    last_ts=datetime.fromtimestamp(desired_start_epoch, tz=timezone.utc),
                )
                db.db.add(st)
                db.db.commit()
                last_ts_epoch = desired_start_epoch
            else:
                # Forward-only: never move last_ts backward
                existing_epoch = (
                    int((st.last_ts if st.last_ts.tzinfo else st.last_ts.replace(tzinfo=timezone.utc)).timestamp())
                    if st.last_ts else None
                )
                new_epoch = desired_start_epoch if existing_epoch is None else max(existing_epoch, desired_start_epoch)
                st.is_running = "true"
                if existing_epoch != new_epoch:
                    st.last_ts = datetime.fromtimestamp(new_epoch, tz=timezone.utc)
                db.db.commit()
                last_ts_epoch = new_epoch

        # Enable auto-advance pacing toggle (does not touch time)
        try:
            import json
            with open("/tmp/sim_auto_advance.json", "w") as f:
                json.dump({"enabled": True, "pace_seconds": float(os.getenv("SIM_PACE_SECONDS", "0"))}, f)
        except Exception:
            pass

        return {"running": True, "last_ts": datetime.fromtimestamp(last_ts_epoch, tz=timezone.utc).isoformat()}
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("start_simulation failed")
        raise HTTPException(status_code=500, detail=str(e))



@router.post("/simulation/stop")
def stop_simulation() -> dict:
    with DBManager() as db:
        user = db.get_user_by_username("analytics")
        if not user:
            return {"running": False}
        st = db.db.query(SimulationState).filter(SimulationState.user_id == user.id).first()
        if not st:
            st = SimulationState(user_id=user.id, is_running="false")
            db.db.add(st)
        else:
            st.is_running = "false"
        db.db.commit()
    try:
        import json
        with open("/tmp/sim_auto_advance.json", "w") as f:
            json.dump({"enabled": False, "stopped": True}, f)
    except Exception:
        pass
    return {"running": False}


@router.get("/simulation/state")
def get_simulation_state() -> dict:
    with DBManager() as db:
        user = db.get_user_by_username("analytics")
        st = db.db.query(SimulationState).filter(SimulationState.user_id == user.id).first() if user else None
    running = False
    if st:
        running = str(st.is_running).lower() in {"true", "1"}
    return {
        "running": running,
        "last_ts": st.last_ts.isoformat() if st and st.last_ts else None,
    }


@router.get("/progress")
def get_progress() -> dict:
    with engine.connect() as conn:
        start_min = conn.execute(select(func.min(HistoricalMinuteBar.ts))).scalar()
        end_min = conn.execute(select(func.max(HistoricalMinuteBar.ts))).scalar()
        start_day = conn.execute(select(func.min(HistoricalDailyBar.date))).scalar()
        end_day = conn.execute(select(func.max(HistoricalDailyBar.date))).scalar()

    sim_ts = _now_sim()
    sim_dt = datetime.fromtimestamp(sim_ts, tz=timezone.utc) if sim_ts else None

    def _ticks(start: Optional[datetime], end: Optional[datetime], step_seconds: int) -> tuple[int, int, float]:
        if not (start and end):
            return (0, 0, 0.0)
        total = max(0, int((end - start).total_seconds() // step_seconds))
        cur = 0 if not sim_dt else max(0, min(total, int((sim_dt - start).total_seconds() // step_seconds)))
        pct = (cur / total * 100.0) if total > 0 else 0.0
        return (cur, total, round(pct, 2))

    cur5, tot5, pct5 = _ticks(start_min, end_min, 300)
    cur1d, tot1d, pct1d = _ticks(start_day, end_day, 86400)

    # light stats
    with DBManager() as db:
        total_exec = db.count_executions()
        total_trades = db.count_trades()

    return {
        "sim_time_epoch": sim_ts,
        "sim_time_iso": sim_dt.isoformat() if sim_dt else None,
        "timeframes": {"5m": {"ticks_done": cur5, "ticks_total": tot5, "percent": pct5},
                       "1d": {"ticks_done": cur1d, "ticks_total": tot1d, "percent": pct1d}},
        "counters": {"executions_all_time": int(total_exec), "trades_all_time": int(total_trades)},
    }


@router.get("/results")
def list_results(
    limit: int = Query(100, ge=1, le=1000),
    strategy: Optional[str] = None,
    symbol: Optional[str] = None,
    timeframe: Optional[str] = None,
) -> list[dict]:
    with DBManager() as db:
        q = db.db.query(AnalyticsResult)
        if strategy:
            q = q.filter(AnalyticsResult.strategy == strategy)
        if symbol:
            q = q.filter(AnalyticsResult.symbol == symbol.upper())
        if timeframe:
            q = q.filter(AnalyticsResult.timeframe == timeframe)
        rows = q.order_by(AnalyticsResult.end_ts.desc().nullslast()).limit(limit).all()
        return [
            {
                "symbol": r.symbol,
                "strategy": r.strategy,
                "timeframe": r.timeframe,
                "start_ts": r.start_ts,
                "end_ts": r.end_ts,
                "final_pnl_amount": r.final_pnl_amount,
                "final_pnl_percent": r.final_pnl_percent,
                "trades_count": r.trades_count,
            }
            for r in rows
        ]


@router.get("/errors")
def list_errors(limit: int = Query(100, ge=1, le=1000)) -> list[dict]:
    with DBManager() as db:
        rows = (
            db.db.query(RunnerExecution)
            .filter(
                (RunnerExecution.status == "error")
                | (RunnerExecution.status == "failed")
                | (RunnerExecution.status.like("skipped%"))
            )
            .order_by(RunnerExecution.execution_time.desc())
            .limit(limit)
            .all()
        )
        return [
            {
                "time": r.execution_time, "runner_id": r.runner_id, "symbol": r.symbol,
                "status": r.status, "reason": r.reason, "details": r.details, "strategy": r.strategy
            }
            for r in rows
        ]


===== backend/api_gateway/security/auth.py =====

DESCRIPTION: Source file

from datetime import datetime, timedelta, timezone
from typing import Optional
import os

from jose import jwt, JWTError          # pip install python-jose
from passlib.context import CryptContext  # pip install passlib[bcrypt]

from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

from database.models import User

# ───── settings (put real values in .env) ────────────────────────────
SECRET_KEY  = os.getenv("AUTH_SECRET_KEY", "CHANGE_ME")
ALGORITHM   = os.getenv("AUTH_ALGORITHM", "HS256")
ACCESS_TTL  = int(os.getenv("AUTH_ACCESS_TTL_MINUTES", str(60 * 24)))  # minutes (default 1 day)

pwd_ctx  = CryptContext(schemes=["bcrypt"], deprecated="auto")
bearer   = HTTPBearer(auto_error=False)  # we’ll raise ourselves

# ───── helpers ───────────────────────────────────────────────────────
def hash_password(pw: str) -> str:
    return pwd_ctx.hash(pw)

def verify_password(plain: str, hashed: str) -> bool:
    return pwd_ctx.verify(plain, hashed)

def create_access_token(sub: str, ttl_minutes: int = ACCESS_TTL) -> str:
    exp = datetime.now(timezone.utc) + timedelta(minutes=ttl_minutes)
    to_encode = {"sub": sub, "exp": exp}
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

def decode_token(token: str) -> Optional[str]:
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        return payload.get("sub")
    except JWTError:
        return None

# ───── FastAPI dependency: current user (rejects 401) ───────────────
def get_current_user(
    cred: HTTPAuthorizationCredentials = Depends(bearer),
) -> User:
    if cred is None:
        raise HTTPException(status.HTTP_401_UNAUTHORIZED, "Not authenticated")

    uid = decode_token(cred.credentials)
    if not uid:
        raise HTTPException(status.HTTP_401_UNAUTHORIZED, "Invalid token")
    
    from database.db_manager import DBManager
    
    try:
        with DBManager() as db:
            user = db.get_user_by_username(uid)
            if user is None:
                raise HTTPException(status.HTTP_401_UNAUTHORIZED, "User not found")
            return user
    except HTTPException:
        raise
    except Exception:
        # Database connection issues should return 503 Service Unavailable
        raise HTTPException(
            status.HTTP_503_SERVICE_UNAVAILABLE, 
            "Authentication service temporarily unavailable"
        )


===== backend/api_gateway/security/__init__.py =====

DESCRIPTION: Source file



===== backend/__init__.py =====

DESCRIPTION: Source file



===== backend/broker/mock_broker.py =====

DESCRIPTION: Source file

from __future__ import annotations

import logging
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Optional, Dict, Any, List

from database.db_manager import DBManager
from database.models import Account, OpenPosition, Order, ExecutedTrade, Runner
from trades_logger import log_buy, log_sell
from backend.ib_manager.market_data_manager import MarketDataManager

log = logging.getLogger("mock-broker")


def _utc(dt: datetime) -> datetime:
    return dt if dt.tzinfo else dt.replace(tzinfo=timezone.utc)


@dataclass(slots=True)
class _RunnerLite:
    id: int
    user_id: int
    stock: str
    parameters: dict


class MockBroker:
    """
    Lightweight fill-at-mid mock broker with:
      • BUY/SELL market/limit fills (assumed immediate at provided `price`)
      • Trailing and static stop management (evaluated on each tick)
      • Mark-to-market account equity (cash + sum(position_value))
    """

    def __init__(self) -> None:
        self.mkt = MarketDataManager()

    # ───────────────────────── helpers ─────────────────────────

    def _runner_lite(self, runner: Any) -> _RunnerLite:
        return _RunnerLite(
            id=int(getattr(runner, "id", 0) or 0),
            user_id=int(getattr(runner, "user_id", 0) or 0),
            stock=str(getattr(runner, "stock", "UNKNOWN") or "UNKNOWN").upper(),
            parameters=dict(getattr(runner, "parameters", {}) or {}),
        )

    def _get_acct(self, db: DBManager, user_id: int) -> Account:
        # Ensure an account exists; RunnerService also ensures, but be defensive.
        return db.ensure_account(user_id=user_id, name="mock")

    # ───────────────────────── public API ─────────────────────────

    def on_tick(self, *, user_id: int, runner: Runner | _RunnerLite, price: float, at: datetime) -> None:
        """
        Update trailing/static stops for the position and execute if hit.
        """
        at = _utc(at)
        rl = self._runner_lite(runner)

        with DBManager() as db:
            pos: Optional[OpenPosition] = (
                db.db.query(OpenPosition)
                .filter(OpenPosition.user_id == user_id, OpenPosition.runner_id == rl.id)
                .first()
            )
            if not pos:
                return

            # Update highest_price for trailing stops
            if (pos.trail_percent or 0) > 0:
                hp = float(pos.highest_price or pos.avg_price or price)
                if price > hp:
                    pos.highest_price = price
                    db.db.commit()

                trail_pct = float(pos.trail_percent or 0.0)
                stop_trail = float((pos.highest_price or hp) * (1.0 - trail_pct / 100.0))
                if price <= stop_trail and pos.quantity > 0:
                    log.info("Trailing stop hit for runner=%s %s at %.4f <= stop %.4f", rl.id, rl.stock, price, stop_trail)
                    self.sell_all(
                        user_id=user_id,
                        runner=runner,
                        symbol=rl.stock,
                        price=price,
                        decision={"reason": "trailing_stop_hit"},
                        at=at,
                        reason_override="trailing_stop_hit",
                    )
                    return  # Position closed; nothing else to check

            # Static stop
            if (pos.stop_price or 0) > 0 and pos.quantity > 0:
                if price <= float(pos.stop_price):
                    log.info("Static stop hit for runner=%s %s at %.4f <= stop %.4f", rl.id, rl.stock, price, float(pos.stop_price))
                    self.sell_all(
                        user_id=user_id,
                        runner=runner,
                        symbol=rl.stock,
                        price=price,
                        decision={"reason": "static_stop_hit"},
                        at=at,
                        reason_override="static_stop_hit",
                    )

    def arm_trailing_stop_once(self, *, user_id: int, runner: Runner | _RunnerLite, entry_price: float, trail_pct: float, at: datetime) -> None:
        """
        If a position exists and has no trailing stop, arm one.
        """
        at = _utc(at)
        rl = self._runner_lite(runner)
        with DBManager() as db:
            pos: Optional[OpenPosition] = (
                db.db.query(OpenPosition)
                .filter(OpenPosition.user_id == user_id, OpenPosition.runner_id == rl.id)
                .first()
            )
            if not pos:
                return
            if (pos.trail_percent or 0) > 0:
                return  # already armed
            pos.trail_percent = float(trail_pct)
            pos.highest_price = float(entry_price)
            db.db.commit()
            log.info("Armed trailing stop %.2f%% for runner=%s %s", float(trail_pct), rl.id, rl.stock)

    def buy(
        self,
        *,
        user_id: int,
        runner: Runner | _RunnerLite,
        symbol: str,
        price: float,
        quantity: int,
        decision: Dict[str, Any] | None,
        at: datetime,
    ) -> bool:
        at = _utc(at)
        rl = self._runner_lite(runner)
        symbol = (symbol or rl.stock).upper()
        if quantity <= 0 or price <= 0:
            return False

        with DBManager() as db:
            acct = self._get_acct(db, user_id)
            cost = float(price) * int(quantity)
            try:
                if float(acct.cash or 0.0) < cost:
                    log.info("BUY rejected (insufficient cash) user=%s runner=%s need=%.2f have=%.2f", user_id, rl.id, cost, float(acct.cash or 0.0))
                    return False
            except Exception:
                return False

            # Upsert position (1 position per runner)
            pos: Optional[OpenPosition] = (
                db.db.query(OpenPosition)
                .filter(OpenPosition.user_id == user_id, OpenPosition.runner_id == rl.id)
                .first()
            )
            if pos:
                # Aggregate into existing position with new avg price
                new_qty = int((pos.quantity or 0) + quantity)
                if new_qty <= 0:
                    return False
                new_cost = (float(pos.avg_price) * int(pos.quantity)) + cost
                pos.quantity = new_qty
                pos.avg_price = new_cost / new_qty
                # Reset stops to the most conservative values from decision (if provided)
            else:
                pos = OpenPosition(
                    user_id=user_id,
                    runner_id=rl.id,
                    symbol=symbol,
                    account="mock",
                    quantity=int(quantity),
                    avg_price=float(price),
                    created_at=at,
                    stop_price=None,
                    trail_percent=None,
                    highest_price=None,
                )
                db.db.add(pos)

            # Apply stop specs from decision
            if isinstance(decision, dict):
                ss = decision.get("static_stop_order") or {}
                tp = decision.get("trail_stop_order") or {}
                try:
                    sp = ss.get("stop_price")
                    if sp is not None and float(sp) > 0:
                        pos.stop_price = float(sp)
                except Exception:
                    pass
                try:
                    trailing_percent = tp.get("trailing_percent")
                    if trailing_percent is not None and float(trailing_percent) > 0:
                        pos.trail_percent = float(trailing_percent)
                        pos.highest_price = max(float(pos.highest_price or 0.0), float(price))
                except Exception:
                    pass

            # Create an order record (filled)
            ord = Order(
                user_id=user_id,
                runner_id=rl.id,
                symbol=symbol,
                side="BUY",
                order_type=(str(decision.get("order_type")) if decision else "MKT"),
                quantity=int(quantity),
                limit_price=decision.get("limit_price") if decision else None,
                stop_price=(decision.get("stop_price") or (decision.get("static_stop_order") or {}).get("stop_price")) if decision else None,
                status="filled",
                created_at=at,
                filled_at=at,
                details=None,
            )
            db.db.add(ord)

            # Deduct cash; equity recalculated in mark_to_market
            acct.cash = float(acct.cash or 0.0) - cost
            db.db.commit()

            log_buy(
                user_id=user_id,
                runner_id=rl.id,
                symbol=symbol,
                qty=float(quantity),
                price=float(price),
                as_of=at,
                reason=str((decision or {}).get("reason") or (decision or {}).get("explanation") or ""),
            )
            log.info("BUY filled user=%s runner=%s %s x%d @ %.4f", user_id, rl.id, symbol, int(quantity), float(price))
            return True

    def sell_all(
        self,
        *,
        user_id: int,
        runner: Runner | _RunnerLite,
        symbol: str,
        price: float,
        decision: Dict[str, Any] | None,
        at: datetime,
        reason_override: Optional[str] = None,
    ) -> bool:
        at = _utc(at)
        rl = self._runner_lite(runner)
        symbol = (symbol or rl.stock).upper()

        with DBManager() as db:
            pos: Optional[OpenPosition] = (
                db.db.query(OpenPosition)
                .filter(OpenPosition.user_id == user_id, OpenPosition.runner_id == rl.id)
                .first()
            )
            if not pos or int(pos.quantity or 0) <= 0:
                return False

            qty = int(pos.quantity)
            avg_price = float(pos.avg_price or 0.0)

            # Create order record (filled)
            ord = Order(
                user_id=user_id,
                runner_id=rl.id,
                symbol=symbol,
                side="SELL",
                order_type=(str(decision.get("order_type")) if decision else "MKT"),
                quantity=qty,
                limit_price=decision.get("limit_price") if decision else None,
                stop_price=decision.get("stop_price") if decision else None,
                status="filled",
                created_at=at,
                filled_at=at,
                details=None,
            )
            db.db.add(ord)

            # Credit cash for the sale
            acct = self._get_acct(db, user_id)
            proceeds = float(price) * qty
            acct.cash = float(acct.cash or 0.0) + proceeds

            # Record execution summary (optional in analytics)
            try:
                db.db.add(
                    ExecutedTrade(
                        user_id=user_id,
                        runner_id=rl.id,
                        symbol=symbol,
                        buy_ts=None,
                        sell_ts=at,
                        buy_price=avg_price,
                        sell_price=float(price),
                        quantity=float(qty),
                        pnl_amount=(float(price) - avg_price) * float(qty),
                        pnl_percent=(0.0 if avg_price == 0 else ((float(price) / avg_price) - 1.0) * 100.0),
                        strategy=None,
                        timeframe=None,
                    )
                )
            except Exception:
                pass

            # Delete the open position
            try:
                db.db.delete(pos)
            except Exception:
                # In rare cases (constraints), zero it out instead
                pos.quantity = 0
            db.db.commit()

            reason = reason_override or str((decision or {}).get("reason") or (decision or {}).get("explanation") or "strategy_sell")
            log_sell(
                user_id=user_id,
                runner_id=rl.id,
                symbol=symbol,
                qty=float(qty),
                avg_price=avg_price,
                price=float(price),
                as_of=at,
                reason=reason,
            )
            log.info("SELL filled user=%s runner=%s %s x%d @ %.4f (%s)", user_id, rl.id, symbol, qty, float(price), reason)
            return True

    def mark_to_market_all(self, *, user_id: int, at: datetime) -> None:
        """
        Refresh account equity and trailing-stop anchors based on last available prices.
        Missing prices are skipped (WARN once); the routine must never raise.
        """
        at = _utc(at)
        with DBManager() as db:
            positions: List[OpenPosition] = (
                db.db.query(OpenPosition)
                .filter(OpenPosition.user_id == user_id)
                .all()
            )
            if not positions:
                # Keep equity == cash if flat
                try:
                    acct = self._get_acct(db, user_id)
                    acct.equity = float(acct.cash or 0.0)
                    db.db.commit()
                except Exception:
                    pass
                return

            syms = [p.symbol.upper() for p in positions]
            try:
                # Use 5-minute prices by default for intraday MTM
                last_px = self.mkt.get_last_close_for_symbols(syms, 5, at, regular_hours_only=True)
            except Exception:
                log.exception("mark_to_market_all: price fetch failed")
                last_px = {}

            # Revalue positions, update trailing anchors, recompute equity
            total_mkt_value = 0.0
            for p in positions:
                px = float(last_px.get(p.symbol.upper(), 0.0) or 0.0)
                if px > 0:
                    total_mkt_value += px * int(p.quantity or 0)

                    # Bump highest_price for trailing stops
                    if (p.trail_percent or 0) > 0:
                        hp = float(p.highest_price or p.avg_price or px)
                        if px > hp:
                            p.highest_price = px
                else:
                    # No price available — skip; don't error
                    pass

            try:
                acct = self._get_acct(db, user_id)
                acct.equity = float(acct.cash or 0.0) + total_mkt_value
                db.db.commit()
            except Exception:
                log.exception("mark_to_market_all: failed to update account equity")


===== /root/projects/SelfTrading Analytics/.env =====

DESCRIPTION: Top-level project file

# ───────── App mode ─────────
RUNNING_ENV=analytics
EXTERNAL_SCHEDULER=1          # api uses external scheduler container

# ───────── Simulation ─────────
SIM_AUTO_START=1              # auto-start after bootstrap completes
SIM_PACE_SECONDS=0            # 0 = full speed (no sleep) in scheduler
SIM_STEP_SECONDS=300          # 5m per tick
SIM_START_CASH=10000000       # starting mock account cash
SIM_SYMBOL_LIMIT=0            # 0 = all symbols found
XRTH_LIMIT_WIGGLE=0.02        # wider LMT wiggle in extended hours (strategies)

# ───────── Database (compose) ─────────
POSTGRES_USER=postgres
POSTGRES_PASSWORD=YUVAL142sabag
POSTGRES_DB=selftrading_analytics_db
POSTGRES_HOST=db
POSTGRES_PORT=5432

# App-side DB settings (kept for completeness; DSN below is authoritative)
DB_USER=postgres
DB_PASSWORD=YUVAL142sabag
DB_ALLOW_AUTO_CREATE_TABLES=true
DB_POOL_SIZE=10
DB_MAX_OVERFLOW=20
DB_POOL_RECYCLE=1800
DB_POOL_TIMEOUT=30
DB_INITIAL_BACKOFF=2
DB_MAX_BACKOFF=60

# Force every service to use the same DSN (avoids fallback mismatches)
DATABASE_URL_DOCKER=postgresql://postgres:YUVAL142sabag@db:5432/selftrading_analytics_db

# ───────── Importer ─────────
ANALYTICS_SQLITE_PATH=/app/tools/finnhub_downloader/data/daily_bars.sqlite

# ───────── Logging ─────────
LOG_DIR=/app/logs
LOG_LEVEL=DEBUG
SIM_LOG_NO_ACTION=1
NO_STDOUT_LOG=0
LOG_STRUCTURED=0
LOG_MAX_BYTES=52428800
LOG_BACKUP_COUNT=10
LOG_MOCK_BROKER_LEVEL=DEBUG

# ───────── Auth (unused in dev) ─────────
AUTH_SECRET_KEY=dev-not-used
AUTH_ALGORITHM=HS256
AUTH_ACCESS_TTL_MINUTES=1440

# ───────── Debug toggles ─────────
DB_DEBUG_ENABLED=true
IB_DEBUG_ENABLED=true

